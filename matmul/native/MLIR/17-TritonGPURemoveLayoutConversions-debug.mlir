Args: build/cmake.linux-x86_64-cpython-3.12/bin/triton-opt /home/ubuntu/triton/matmul.mlir --tritongpu-remove-layout-conversions -o a.mlir --debug
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorElementTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FloatType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PtrLikeTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BlobAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<Empty>)
Load new dialect in Context ttg
Load new dialect in Context tt
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithRoundingModeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SelectLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToEmitCPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferDeallocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
Load new dialect in Context math
Load new dialect in Context scf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ParallelCombiningOpInterface)
Load new dialect in Context cf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::WeightedBranchOpInterface)
Load new dialect in Context ub
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ub::PoisonAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DescriptorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DescriptorStoreLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DotOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::TransposeOpInterface)
Load new dialect in Context gpu
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::AsyncTokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::MMAMatrixType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseDnTensorHandleType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseSpMatHandleType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseSpGEMMOpHandleType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::AsyncOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DeviceMappingAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::TritonGPU_AttrTrait)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::SharedEncodingTrait)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::LayoutEncodingTrait)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::DistributedEncodingTrait)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::MmaEncodingTrait)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DialectInferLayoutInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DialectVerifyTensorLayoutInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::TensorOrMemDesc)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::ModuleOp>::Impl<mlir::TypeID::get<mlir::OpTrait::HasParent<mlir::ModuleOp>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<mlir::TypeID::get<mlir::BytecodeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<mlir::TypeID::get<mlir::OpTrait::AutomaticAllocationScope>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<mlir::TypeID::get<mlir::CallableOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<mlir::TypeID::get<mlir::SymbolOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<mlir::TypeID::get<mlir::FunctionOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<mlir::TypeID::get<mlir::OpAsmOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::TensorSizeTrait<mlir::TypeID::get<mlir::OpTrait::TensorSizeTrait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VerifyTensorLayoutsTrait<mlir::TypeID::get<mlir::OpTrait::VerifyTensorLayoutsTrait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VerifiableTensorEncoding)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::ConstantOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<mlir::TypeID::get<mlir::ConditionallySpeculatable::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<mlir::TypeID::get<mlir::MemoryEffectOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<mlir::TypeID::get<mlir::InferTypeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::detail::GetProgramIdOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsCommutative<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultType<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Elementwise<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Scalarizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Vectorizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Tensorizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<mlir::TypeID::get<mlir::OpTrait::OneOperand>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultElementType<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultElementType>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultEncoding<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultEncoding>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultShape<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultShape>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::Type>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<mlir::TypeID::get<mlir::OpTrait::NOperands<2>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicResults<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<3>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::scf::YieldOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<1>::Impl<mlir::TypeID::get<mlir::OpTrait::AtLeastNOperands<1>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AttrSizedOperandSegments<mlir::TypeID::get<mlir::OpTrait::AttrSizedOperandSegments>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsAndResultShape<mlir::TypeID::get<mlir::OpTrait::SameLoadStoreOperandsAndResultShape>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsAndResultEncoding<mlir::TypeID::get<mlir::OpTrait::SameLoadStoreOperandsAndResultEncoding>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::detail::LoadOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::scf::ExecuteRegionOp, mlir::scf::ForOp, mlir::scf::IfOp, mlir::scf::IndexSwitchOp, mlir::scf::WhileOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameTypeOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::CmpIOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueSemantics<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIdempotent<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<2>::Impl<mlir::TypeID::get<mlir::OpTrait::AtLeastNOperands<2>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsShape<mlir::TypeID::get<mlir::OpTrait::SameLoadStoreOperandsShape>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsEncoding<mlir::TypeID::get<mlir::OpTrait::SameLoadStoreOperandsEncoding>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::triton::FuncOp>::Impl<mlir::TypeID::get<mlir::OpTrait::HasParent<mlir::triton::FuncOp>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<mlir::TypeID::get<mlir::RegionBranchTerminatorOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::AttributeTrait::IsLocation<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutSpecInterface)
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1, 0)
   warp=2 -> (2, 0)
where out dims are: [dim0 (size 4), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim0 (size 32), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim0 (size 32), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim1 (size 1), dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim1 (size 1), dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim1 (size 1), dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim0 (size 1), dim1 (size 4)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim0 (size 1), dim1 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim0 (size 1), dim1 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim0 (size 1), dim1 (size 128)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim0 (size 1), dim1 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim0 (size 1), dim1 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 0)
where out dims are: [dim0 (size 1), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim1 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim1 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim0 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 2), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim1 (size 32), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim1 (size 32), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 0)
where out dims are: [dim1 (size 64), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim1 (size 64), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim1 (size 1), dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim1 (size 1), dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim1 (size 1), dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
   register=16 -> (16)
   register=32 -> (32)
where out dims are: [dim1 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
 - block is a size 1 dimension
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 1)
   register=2 -> (0, 2)
   register=4 -> (0, 4)
   register=8 -> (0, 8)
   register=16 -> (0, 16)
   register=32 -> (0, 32)
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (64, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim1 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim1 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim0 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 2), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim1 (size 32), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim1 (size 32), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
   register=16 -> (16)
   register=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 2)
   register=2 -> (0, 4)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 2)
   register=2 -> (0, 4)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 2)
   register=2 -> (0, 4)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
 - block is a size 1 dimension
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (2, 0)
   register=2 -> (4, 0)
   register=4 -> (8, 0)
   register=8 -> (16, 0)
   register=16 -> (32, 0)
   register=32 -> (64, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (1, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim1 (size 1), dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
where out dims are: [dim1 (size 16)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
where out dims are: [dim1 (size 16)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
where out dims are: [dim0 (size 2)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (0, 1)
where out dims are: [dim1 (size 16), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
where out dims are: [dim1 (size 4), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
where out dims are: [dim0 (size 16)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
 - block is a size 1 dimension
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 1)
   register=2 -> (0, 2)
   register=4 -> (8, 0)
   register=8 -> (16, 0)
   register=16 -> (32, 0)
   register=32 -> (64, 0)
 - lane=1 -> (0, 4)
   lane=2 -> (0, 8)
   lane=4 -> (0, 16)
   lane=8 -> (0, 32)
   lane=16 -> (1, 0)
 - warp=1 -> (2, 0)
   warp=2 -> (4, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 64)]
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::OpToOpPassAdaptor)
[tritongpu-remove-layout-conversions]: propagateLayout considering ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmTypeInterface)
%45 = tt.load %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 1
[tritongpu-remove-layout-conversions]: propagateLayout considering %51 = tt.broadcast %45 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 1
[tritongpu-remove-layout-conversions]: propagateLayout considering %52 = ttg.convert_layout %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 1
[tritongpu-remove-layout-conversions]: propagateLayout considering %54 = arith.mulf %52, %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 1
[tritongpu-remove-layout-conversions]: propagateLayout considering %55 = arith.addf %arg10, %54 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 2
[tritongpu-remove-layout-conversions]: propagateLayout considering <block argument> of type 'tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>' at index: 1, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 0
[tritongpu-remove-layout-conversions]: propagateLayout considering %22 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>)  : i32 {
  %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %43 = tt.addptr %16, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %44 = ttg.convert_layout %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %45 = tt.load %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %46 = arith.muli %arg9, %arg7 : i32
  %47 = tt.splat %46 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %48 = tt.addptr %21, %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %49 = ttg.convert_layout %48 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %50 = tt.load %49 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %51 = tt.broadcast %45 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %52 = ttg.convert_layout %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %53 = tt.broadcast %50 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %54 = arith.mulf %52, %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %55 = arith.addf %arg10, %54 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  scf.yield %55 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
}, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 1
[tritongpu-remove-layout-conversions]: propagateLayout considering %40 = ttg.convert_layout %22 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, which has 1 candidate encoding(s):
[tritongpu-remove-layout-conversions]:   #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]: changed: 0
[tritongpu-remove-layout-conversions]: Module after propagating layouts forward:
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %0 = ttg.convert_layout %cst : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %c1_i32 = arith.constant 1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %1 = tt.get_program_id x : i32
    %2 = tt.get_program_id y : i32
    %3 = arith.muli %2, %c128_i32 : i32
    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %5 = tt.splat %3 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %6 = arith.addi %5, %4 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %7 = arith.muli %1, %c64_i32 : i32
    %8 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %9 = tt.splat %7 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %10 = arith.addi %9, %8 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %11 = ttg.convert_layout %6 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
    %12 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
    %13 = ttg.convert_layout %12 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %14 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %15 = arith.muli %13, %14 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %16 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %17 = tt.addptr %16, %15 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %18 = ttg.convert_layout %10 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
    %19 = tt.expand_dims %18 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
    %20 = ttg.convert_layout %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %21 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %22 = tt.addptr %21, %20 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %23 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %0) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
      %44 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %45 = tt.addptr %17, %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %46 = ttg.convert_layout %45 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %47 = tt.load %46 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %48 = arith.muli %arg9, %arg7 : i32
      %49 = tt.splat %48 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %50 = tt.addptr %22, %49 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %51 = ttg.convert_layout %50 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %52 = tt.load %51 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %53 = tt.broadcast %47 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %54 = ttg.convert_layout %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %55 = tt.broadcast %52 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %56 = ttg.convert_layout %55 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %57 = arith.mulf %54, %56 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %58 = arith.addf %arg10, %57 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      scf.yield %58 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    }
    %24 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %25 = arith.muli %13, %24 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %26 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %27 = tt.addptr %26, %25 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %28 = tt.broadcast %27 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %29 = ttg.convert_layout %28 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %30 = tt.broadcast %20 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %31 = tt.addptr %29, %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %32 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %33 = arith.cmpi slt, %13, %32 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %34 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %35 = arith.cmpi slt, %20, %34 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %36 = tt.broadcast %33 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %37 = ttg.convert_layout %36 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %38 = tt.broadcast %35 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %39 = arith.andi %37, %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %40 = ttg.convert_layout %31 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %41 = ttg.convert_layout %23 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %42 = ttg.convert_layout %41 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %43 = ttg.convert_layout %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.store %40, %42, %43 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.return
  }
}
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x56020434b210) {
  "tt.return"() : () -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x560204387c60) {
  "tt.store"(%45, %47, %48) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::GlobalMemory)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Write)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Allocate)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Read)
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204393080) {
  %48 = "ttg.convert_layout"(%44) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020439bfe0) {
  %47 = "ttg.convert_layout"(%46) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
    ** Insert  : 'ttg.convert_layout'(0x560204392f90)
    ** Replace : 'ttg.convert_layout'(0x56020439bfe0)
    ** Modified: 'tt.store'(0x560204387c60)
    ** Erase   : 'ttg.convert_layout'(0x56020439bfe0)
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c128_i32 = arith.constant 128 : i32
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %0 = ttg.convert_layout %cst : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %1 = tt.get_program_id x : i32
  %2 = tt.get_program_id y : i32
  %3 = arith.muli %2, %c128_i32 : i32
  %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %5 = tt.splat %3 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %6 = arith.addi %5, %4 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %7 = arith.muli %1, %c64_i32 : i32
  %8 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %9 = tt.splat %7 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %10 = arith.addi %9, %8 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %11 = ttg.convert_layout %6 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
  %12 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
  %13 = ttg.convert_layout %12 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %14 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %15 = arith.muli %13, %14 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %16 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %17 = tt.addptr %16, %15 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %18 = ttg.convert_layout %10 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
  %19 = tt.expand_dims %18 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
  %20 = ttg.convert_layout %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %21 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %22 = tt.addptr %21, %20 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %23 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %0) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
    %44 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %45 = tt.addptr %17, %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %46 = ttg.convert_layout %45 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %47 = tt.load %46 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %48 = arith.muli %arg9, %arg7 : i32
    %49 = tt.splat %48 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %50 = tt.addptr %22, %49 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %51 = ttg.convert_layout %50 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %52 = tt.load %51 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %53 = tt.broadcast %47 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %54 = ttg.convert_layout %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %55 = tt.broadcast %52 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %56 = ttg.convert_layout %55 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %57 = arith.mulf %54, %56 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %58 = arith.addf %arg10, %57 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    scf.yield %58 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  }
  %24 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %25 = arith.muli %13, %24 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %26 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %27 = tt.addptr %26, %25 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %28 = tt.broadcast %27 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %29 = ttg.convert_layout %28 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %30 = tt.broadcast %20 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %31 = tt.addptr %29, %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %32 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %33 = arith.cmpi slt, %13, %32 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %34 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %35 = arith.cmpi slt, %20, %34 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %36 = tt.broadcast %33 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %37 = ttg.convert_layout %36 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %38 = tt.broadcast %35 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %39 = arith.andi %37, %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %40 = ttg.convert_layout %31 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %41 = ttg.convert_layout %23 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %42 = ttg.convert_layout %23 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %43 = ttg.convert_layout %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.store %40, %42, %43 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.return
}


} -> success : at least one pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x560204387c60) {
  "tt.store"(%45, %47, %48) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392f90) {
  %47 = "ttg.convert_layout"(%28) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020439b5e0) {
  %46 = "ttg.convert_layout"(%28) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'ttg.convert_layout'(0x56020439b5e0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392ec0) {
  %45 = "ttg.convert_layout"(%36) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x560204392dd0) {
  %44 = "arith.andi"(%42, %43) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392d40) {
  %43 = "tt.broadcast"(%40) : (tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392cb0) {
  %42 = "ttg.convert_layout"(%41) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392c20) {
  %41 = "tt.broadcast"(%38) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x560204392550) {
  %40 = "arith.cmpi"(%25, %39) <{predicate = 2 : i64}> : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043924c0) {
  %39 = "tt.splat"(%arg5) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x560204392410) {
  %38 = "arith.cmpi"(%18, %37) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204392380) {
  %37 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043922d0) {
  %36 = "tt.addptr"(%34, %35) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392240) {
  %35 = "tt.broadcast"(%25) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x5602043921b0) {
  %34 = "ttg.convert_layout"(%33) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392120) {
  %33 = "tt.broadcast"(%32) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x560204392070) {
  %32 = "tt.addptr"(%31, %30) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204391fe0) {
  %31 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204391790) {
  %30 = "arith.muli"(%18, %29) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204391700) {
  %29 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x560204391f50) {
  "scf.yield"(%62) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5602043852e0) {
  %62 = "arith.addf"(%arg10, %61) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x560204385230) {
  %61 = "arith.mulf"(%58, %60) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020439beb0) {
  %60 = "ttg.convert_layout"(%59) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020438f060) {
  %59 = "tt.broadcast"(%56) : (tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020439ddd0) {
  %58 = "ttg.convert_layout"(%57) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
    ** Replace : 'ttg.convert_layout'(0x56020439ddd0)
    ** Modified: 'arith.mulf'(0x560204385230)
    ** Erase   : 'ttg.convert_layout'(0x56020439ddd0)
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c128_i32 = arith.constant 128 : i32
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %0 = ttg.convert_layout %cst : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %1 = tt.get_program_id x : i32
  %2 = tt.get_program_id y : i32
  %3 = arith.muli %2, %c128_i32 : i32
  %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %5 = tt.splat %3 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %6 = arith.addi %5, %4 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %7 = arith.muli %1, %c64_i32 : i32
  %8 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %9 = tt.splat %7 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %10 = arith.addi %9, %8 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %11 = ttg.convert_layout %6 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
  %12 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
  %13 = ttg.convert_layout %12 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %14 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %15 = arith.muli %13, %14 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %16 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %17 = tt.addptr %16, %15 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %18 = ttg.convert_layout %10 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
  %19 = tt.expand_dims %18 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
  %20 = ttg.convert_layout %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %21 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %22 = tt.addptr %21, %20 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %23 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %0) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
    %43 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %44 = tt.addptr %17, %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %45 = ttg.convert_layout %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %46 = tt.load %45 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %47 = arith.muli %arg9, %arg7 : i32
    %48 = tt.splat %47 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %49 = tt.addptr %22, %48 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %50 = ttg.convert_layout %49 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %51 = tt.load %50 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %52 = tt.broadcast %46 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %53 = tt.broadcast %51 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %54 = ttg.convert_layout %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %55 = arith.mulf %52, %54 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %56 = arith.addf %arg10, %55 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    scf.yield %56 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  }
  %24 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %25 = arith.muli %13, %24 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %26 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %27 = tt.addptr %26, %25 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %28 = tt.broadcast %27 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %29 = ttg.convert_layout %28 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %30 = tt.broadcast %20 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %31 = tt.addptr %29, %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %32 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %33 = arith.cmpi slt, %13, %32 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %34 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %35 = arith.cmpi slt, %20, %34 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %36 = tt.broadcast %33 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %37 = ttg.convert_layout %36 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %38 = tt.broadcast %35 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %39 = arith.andi %37, %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %40 = ttg.convert_layout %31 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %41 = ttg.convert_layout %23 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %42 = ttg.convert_layout %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.store %40, %41, %42 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.return
}


} -> success : at least one pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x560204385230) {
  %60 = "arith.mulf"(%57, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204390f00) {
  %57 = "tt.broadcast"(%51) : (tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5602043909a0) {
  %56 = "tt.load"(%55) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204390870) {
  %55 = "ttg.convert_layout"(%54) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
    ** Replace : 'ttg.convert_layout'(0x560204390870)
    ** Modified: 'tt.load'(0x5602043909a0)
    ** Erase   : 'ttg.convert_layout'(0x560204390870)
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c128_i32 = arith.constant 128 : i32
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %0 = ttg.convert_layout %cst : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %1 = tt.get_program_id x : i32
  %2 = tt.get_program_id y : i32
  %3 = arith.muli %2, %c128_i32 : i32
  %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %5 = tt.splat %3 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %6 = arith.addi %5, %4 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %7 = arith.muli %1, %c64_i32 : i32
  %8 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %9 = tt.splat %7 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %10 = arith.addi %9, %8 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %11 = ttg.convert_layout %6 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
  %12 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
  %13 = ttg.convert_layout %12 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %14 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %15 = arith.muli %13, %14 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %16 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %17 = tt.addptr %16, %15 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %18 = ttg.convert_layout %10 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
  %19 = tt.expand_dims %18 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
  %20 = ttg.convert_layout %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %21 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %22 = tt.addptr %21, %20 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %23 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %0) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
    %43 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %44 = tt.addptr %17, %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %45 = ttg.convert_layout %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %46 = tt.load %45 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %47 = arith.muli %arg9, %arg7 : i32
    %48 = tt.splat %47 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %49 = tt.addptr %22, %48 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %50 = tt.load %49 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %51 = tt.broadcast %46 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %52 = tt.broadcast %50 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %53 = ttg.convert_layout %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %54 = arith.mulf %51, %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %55 = arith.addf %arg10, %54 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    scf.yield %55 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  }
  %24 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %25 = arith.muli %13, %24 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %26 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %27 = tt.addptr %26, %25 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %28 = tt.broadcast %27 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %29 = ttg.convert_layout %28 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %30 = tt.broadcast %20 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %31 = tt.addptr %29, %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %32 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %33 = arith.cmpi slt, %13, %32 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %34 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %35 = arith.cmpi slt, %20, %34 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %36 = tt.broadcast %33 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %37 = ttg.convert_layout %36 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %38 = tt.broadcast %35 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %39 = arith.andi %37, %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %40 = ttg.convert_layout %31 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %41 = ttg.convert_layout %23 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %42 = ttg.convert_layout %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.store %40, %41, %42 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.return
}


} -> success : at least one pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5602043909a0) {
  %55 = "tt.load"(%54) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x560204390760) {
  %54 = "tt.addptr"(%27, %53) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204390060) {
  %53 = "tt.splat"(%52) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438ff50) {
  %52 = "arith.muli"(%arg9, %arg7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020438f9f0) {
  %51 = "tt.load"(%50) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438f440) {
  %50 = "ttg.convert_layout"(%49) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
    ** Replace : 'ttg.convert_layout'(0x56020438f440)
    ** Modified: 'tt.load'(0x56020438f9f0)
    ** Erase   : 'ttg.convert_layout'(0x56020438f440)
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c128_i32 = arith.constant 128 : i32
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %0 = ttg.convert_layout %cst : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %1 = tt.get_program_id x : i32
  %2 = tt.get_program_id y : i32
  %3 = arith.muli %2, %c128_i32 : i32
  %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %5 = tt.splat %3 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %6 = arith.addi %5, %4 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %7 = arith.muli %1, %c64_i32 : i32
  %8 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %9 = tt.splat %7 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %10 = arith.addi %9, %8 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %11 = ttg.convert_layout %6 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
  %12 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
  %13 = ttg.convert_layout %12 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %14 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %15 = arith.muli %13, %14 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %16 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %17 = tt.addptr %16, %15 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %18 = ttg.convert_layout %10 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
  %19 = tt.expand_dims %18 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
  %20 = ttg.convert_layout %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %21 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %22 = tt.addptr %21, %20 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %23 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %0) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
    %43 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %44 = tt.addptr %17, %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %45 = tt.load %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %46 = arith.muli %arg9, %arg7 : i32
    %47 = tt.splat %46 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %48 = tt.addptr %22, %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %49 = tt.load %48 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %50 = tt.broadcast %45 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %51 = tt.broadcast %49 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %52 = ttg.convert_layout %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %53 = arith.mulf %50, %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %54 = arith.addf %arg10, %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    scf.yield %54 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  }
  %24 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %25 = arith.muli %13, %24 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %26 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %27 = tt.addptr %26, %25 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %28 = tt.broadcast %27 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %29 = ttg.convert_layout %28 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %30 = tt.broadcast %20 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %31 = tt.addptr %29, %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %32 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %33 = arith.cmpi slt, %13, %32 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %34 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %35 = arith.cmpi slt, %20, %34 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %36 = tt.broadcast %33 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %37 = ttg.convert_layout %36 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %38 = tt.broadcast %35 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %39 = arith.andi %37, %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %40 = ttg.convert_layout %31 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %41 = ttg.convert_layout %23 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %42 = ttg.convert_layout %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.store %40, %41, %42 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.return
}


} -> success : at least one pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020438f9f0) {
  %50 = "tt.load"(%49) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438f330) {
  %49 = "tt.addptr"(%22, %48) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x560204398500) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438f240) {
  %48 = "tt.splat"(%arg9) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438ee80) {
  %27 = "tt.addptr"(%26, %25) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438ed90) {
  %26 = "tt.splat"(%arg1) : (!tt.ptr<f32>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438e820) {
  %25 = "ttg.convert_layout"(%24) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x56020438e730) {
  %24 = "tt.expand_dims"(%23) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438e640) {
  %23 = "ttg.convert_layout"(%15) : (tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438e0b0) {
  %22 = "tt.addptr"(%21, %20) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438dfc0) {
  %21 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438da30) {
  %20 = "arith.muli"(%18, %19) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438d520) {
  %19 = "tt.splat"(%arg6) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204387710) {
  %18 = "ttg.convert_layout"(%17) : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043875b0) {
  %17 = "tt.expand_dims"(%16) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438b5e0) {
  %16 = "ttg.convert_layout"(%11) : (tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x56020438a750) {
  %15 = "arith.addi"(%14, %13) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>, tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438a660) {
  %14 = "tt.splat"(%12) : (i32) -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020438a110) {
  %13 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204389280) {
  %12 = "arith.muli"(%6, %1) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204388cf0) {
  %11 = "arith.addi"(%10, %9) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>, tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204381bd0) {
  %10 = "tt.splat"(%8) : (i32) -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x560204388bc0) {
  %9 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204388630) {
  %8 = "arith.muli"(%7, %0) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x560204355130) {
  %7 = "tt.get_program_id"() <{axis = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5602043540a0) {
  %6 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204385500) {
  %0 = "arith.constant"() <{value = 128 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386ff0) {
  %1 = "arith.constant"() <{value = 64 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386a70) {
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020439da70) {
  %5 = "ttg.convert_layout"(%4) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
    ** Insert  : 'arith.constant'(0x5602043a54e0)
    ** Replace : 'ttg.convert_layout'(0x56020439da70)
    ** Modified: 'scf.for'(0x560204398500)
    ** Erase   : 'ttg.convert_layout'(0x56020439da70)
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c128_i32 = arith.constant 128 : i32
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %0 = tt.get_program_id x : i32
  %1 = tt.get_program_id y : i32
  %2 = arith.muli %1, %c128_i32 : i32
  %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %4 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %5 = arith.addi %4, %3 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %6 = arith.muli %0, %c64_i32 : i32
  %7 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %8 = tt.splat %6 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %9 = arith.addi %8, %7 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
  %10 = ttg.convert_layout %5 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
  %11 = tt.expand_dims %10 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
  %12 = ttg.convert_layout %11 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %13 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %14 = arith.muli %12, %13 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %15 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %16 = tt.addptr %15, %14 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %17 = ttg.convert_layout %9 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
  %18 = tt.expand_dims %17 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
  %19 = ttg.convert_layout %18 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %20 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %21 = tt.addptr %20, %19 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %22 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst_0) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
    %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %43 = tt.addptr %16, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %45 = arith.muli %arg9, %arg7 : i32
    %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %47 = tt.addptr %21, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %51 = ttg.convert_layout %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %52 = arith.mulf %49, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %53 = arith.addf %arg10, %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    scf.yield %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  }
  %23 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %24 = arith.muli %12, %23 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %25 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %26 = tt.addptr %25, %24 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %27 = tt.broadcast %26 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %28 = ttg.convert_layout %27 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %29 = tt.broadcast %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %30 = tt.addptr %28, %29 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %31 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %32 = arith.cmpi slt, %12, %31 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %33 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %34 = arith.cmpi slt, %19, %33 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %35 = tt.broadcast %32 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %36 = ttg.convert_layout %35 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %37 = tt.broadcast %34 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %38 = arith.andi %36, %37 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %39 = ttg.convert_layout %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %40 = ttg.convert_layout %22 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %41 = ttg.convert_layout %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.store %39, %40, %41 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
  tt.return
}


} -> success : at least one pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x560204398500) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5602043a54e0) {
  %5 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>}> : () -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x560204392690) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386410) {
  %4 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>}> : () -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'arith.constant'(0x560204386410)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x56020434b210) {
  "tt.return"() : () -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x560204387c60) {
  "tt.store"(%44, %45, %46) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204393080) {
  %46 = "ttg.convert_layout"(%43) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392f90) {
  %45 = "ttg.convert_layout"(%27) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392ec0) {
  %44 = "ttg.convert_layout"(%35) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x560204392dd0) {
  %43 = "arith.andi"(%41, %42) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392d40) {
  %42 = "tt.broadcast"(%39) : (tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392cb0) {
  %41 = "ttg.convert_layout"(%40) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392c20) {
  %40 = "tt.broadcast"(%37) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x560204392550) {
  %39 = "arith.cmpi"(%24, %38) <{predicate = 2 : i64}> : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043924c0) {
  %38 = "tt.splat"(%arg5) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x560204392410) {
  %37 = "arith.cmpi"(%17, %36) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204392380) {
  %36 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043922d0) {
  %35 = "tt.addptr"(%33, %34) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392240) {
  %34 = "tt.broadcast"(%24) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x5602043921b0) {
  %33 = "ttg.convert_layout"(%32) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392120) {
  %32 = "tt.broadcast"(%31) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x560204392070) {
  %31 = "tt.addptr"(%30, %29) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204391fe0) {
  %30 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204391790) {
  %29 = "arith.muli"(%17, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204391700) {
  %28 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x560204391f50) {
  "scf.yield"(%58) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5602043852e0) {
  %58 = "arith.addf"(%arg10, %57) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x560204385230) {
  %57 = "arith.mulf"(%54, %56) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020439beb0) {
  %56 = "ttg.convert_layout"(%55) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020438f060) {
  %55 = "tt.broadcast"(%53) : (tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204390f00) {
  %54 = "tt.broadcast"(%49) : (tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5602043909a0) {
  %53 = "tt.load"(%52) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x560204390760) {
  %52 = "tt.addptr"(%26, %51) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204390060) {
  %51 = "tt.splat"(%50) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438ff50) {
  %50 = "arith.muli"(%arg9, %arg7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020438f9f0) {
  %49 = "tt.load"(%48) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438f330) {
  %48 = "tt.addptr"(%21, %47) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x560204398500) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438f240) {
  %47 = "tt.splat"(%arg9) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438ee80) {
  %26 = "tt.addptr"(%25, %24) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438ed90) {
  %25 = "tt.splat"(%arg1) : (!tt.ptr<f32>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438e820) {
  %24 = "ttg.convert_layout"(%23) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x56020438e730) {
  %23 = "tt.expand_dims"(%22) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438e640) {
  %22 = "ttg.convert_layout"(%14) : (tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438e0b0) {
  %21 = "tt.addptr"(%20, %19) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438dfc0) {
  %20 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438da30) {
  %19 = "arith.muli"(%17, %18) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438d520) {
  %18 = "tt.splat"(%arg6) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204387710) {
  %17 = "ttg.convert_layout"(%16) : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043875b0) {
  %16 = "tt.expand_dims"(%15) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x56020438b5e0) {
  %15 = "ttg.convert_layout"(%10) : (tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x56020438a750) {
  %14 = "arith.addi"(%13, %12) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>, tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438a660) {
  %13 = "tt.splat"(%11) : (i32) -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020438a110) {
  %12 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204389280) {
  %11 = "arith.muli"(%5, %1) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204388cf0) {
  %10 = "arith.addi"(%9, %8) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>, tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204381bd0) {
  %9 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x560204388bc0) {
  %8 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204388630) {
  %7 = "arith.muli"(%6, %0) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x560204355130) {
  %6 = "tt.get_program_id"() <{axis = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5602043540a0) {
  %5 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5602043a54e0) {
  %4 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>}> : () -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386a70) {
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386ff0) {
  %1 = "arith.constant"() <{value = 64 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x560204392690) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204385500) {
  %0 = "arith.constant"() <{value = 128 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//
[tritongpu-remove-layout-conversions]: Module after canonicalizing:
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.muli %1, %c128_i32 : i32
    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %4 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %5 = arith.addi %4, %3 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %6 = arith.muli %0, %c64_i32 : i32
    %7 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %8 = tt.splat %6 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %9 = arith.addi %8, %7 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %10 = ttg.convert_layout %5 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
    %11 = tt.expand_dims %10 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
    %12 = ttg.convert_layout %11 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %13 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %14 = arith.muli %12, %13 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %15 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %16 = tt.addptr %15, %14 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %17 = ttg.convert_layout %9 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
    %18 = tt.expand_dims %17 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
    %19 = ttg.convert_layout %18 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %20 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %21 = tt.addptr %20, %19 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %22 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
      %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %43 = tt.addptr %16, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %45 = arith.muli %arg9, %arg7 : i32
      %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %47 = tt.addptr %21, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %51 = ttg.convert_layout %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %52 = arith.mulf %49, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %53 = arith.addf %arg10, %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      scf.yield %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    }
    %23 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %24 = arith.muli %12, %23 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %25 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %26 = tt.addptr %25, %24 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %27 = tt.broadcast %26 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %28 = ttg.convert_layout %27 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %29 = tt.broadcast %19 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %30 = tt.addptr %28, %29 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %31 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %32 = arith.cmpi slt, %12, %31 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %33 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %34 = arith.cmpi slt, %19, %33 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %35 = tt.broadcast %32 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %36 = ttg.convert_layout %35 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %37 = tt.broadcast %34 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %38 = arith.andi %36, %37 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %39 = ttg.convert_layout %30 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %40 = ttg.convert_layout %22 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %41 = ttg.convert_layout %38 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.store %39, %40, %41 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.return
  }
}
[tritongpu-remove-layout-conversions]: check backward remat with source %5 = arith.addi %4, %3 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 16384
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 0
[tritongpu-remove-layout-conversions]:   remat convert op %10 = ttg.convert_layout %5 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]:     %5 = arith.addi %4, %3 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
[tritongpu-remove-layout-conversions]:     %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
[tritongpu-remove-layout-conversions]:     %4 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}> %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %6 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}> %5 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %8 = arith.addi %6, %4 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}> %7 = arith.addi %5, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %14 = tt.expand_dims %7 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 16384
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 0
[tritongpu-remove-layout-conversions]:   remat convert op %15 = ttg.convert_layout %14 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %14 = tt.expand_dims %7 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
[tritongpu-remove-layout-conversions]:     %7 = arith.addi %5, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]:     %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]:     %5 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (64)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
%4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}> %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %7 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}> %6 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %10 = arith.addi %7, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}> %9 = arith.addi %6, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %18 = tt.expand_dims %10 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %17 = tt.expand_dims %9 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %15 = arith.addi %14, %13 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 8192
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 0
[tritongpu-remove-layout-conversions]:   remat convert op %24 = ttg.convert_layout %15 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]:     %15 = arith.addi %14, %13 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
[tritongpu-remove-layout-conversions]:     %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
[tritongpu-remove-layout-conversions]:     %14 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %14 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}> %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %16 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}> %15 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %18 = arith.addi %16, %14 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}> %17 = arith.addi %15, %13 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %28 = tt.expand_dims %17 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 8192
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 0
[tritongpu-remove-layout-conversions]:   remat convert op %29 = ttg.convert_layout %28 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %28 = tt.expand_dims %17 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
[tritongpu-remove-layout-conversions]:     %17 = arith.addi %15, %13 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]:     %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]:     %15 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
 - warp=1 -> (32)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
%14 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}> %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %17 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}> %16 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %20 = arith.addi %17, %14 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}> %19 = arith.addi %16, %13 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %32 = tt.expand_dims %20 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %31 = tt.expand_dims %19 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %64 = tt.broadcast %62 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 1048576
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 256
[tritongpu-remove-layout-conversions]:   remat convert op %65 = ttg.convert_layout %64 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %64 = tt.broadcast %62 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %62 = tt.load %61 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %61 = tt.addptr %35, %60 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %60 = tt.splat %59 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %35 = tt.addptr %34, %31 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %31 = tt.expand_dims %19 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %19 = arith.addi %16, %13 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %16 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %34 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim1 (size 1), dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim0 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim1 (size 1), dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
where out dims are: [dim1 (size 1), dim0 (size 32)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
   register=16 -> (16)
   register=32 -> (32)
where out dims are: [dim1 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 1)
   lane=2 -> (0, 2)
   lane=4 -> (0, 4)
   lane=8 -> (0, 8)
   lane=16 -> (0, 16)
 - warp=1 -> (0, 32)
   warp=2 -> (0, 64)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 0)
where out dims are: [dim1 (size 64), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (4, 0)
   register=8 -> (8, 0)
   register=16 -> (16, 0)
   register=32 -> (32, 0)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim1 (size 64), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 1)
   register=2 -> (0, 2)
   register=4 -> (0, 4)
   register=8 -> (0, 8)
   register=16 -> (0, 16)
   register=32 -> (0, 32)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 64)]
%35 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %34 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
   register=16 -> (16)
   register=32 -> (32)
 - lane=1 -> (0)
   lane=2 -> (0)
   lane=4 -> (0)
   lane=8 -> (0)
   lane=16 -> (0)
 - warp=1 -> (0)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
   register=16 -> (16)
   register=32 -> (32)
 - lane=1 -> (0)
   lane=2 -> (0)
   lane=4 -> (0)
   lane=8 -> (0)
   lane=16 -> (0)
 - warp=1 -> (0)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
%14 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}> %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %63 = tt.splat %61 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %62 = tt.splat %61 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %18 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}> %17 = tt.splat %12 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %22 = arith.addi %18, %14 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}> %21 = arith.addi %17, %13 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %35 = tt.expand_dims %22 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %34 = tt.expand_dims %21 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %41 = tt.addptr %39, %35 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %40 = tt.addptr %38, %34 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %69 = tt.addptr %41, %67 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %68 = tt.addptr %40, %66 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %71 = tt.load %69 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %70 = tt.load %68 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %74 = tt.broadcast %71 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}> %73 = tt.broadcast %70 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %47 = tt.broadcast %46 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 1048576
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 512
[tritongpu-remove-layout-conversions]:   remat convert op %48 = ttg.convert_layout %47 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %47 = tt.broadcast %46 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %46 = tt.addptr %45, %44 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %44 = arith.muli %26, %43 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %43 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %26 = tt.expand_dims %9 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %9 = arith.addi %6, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %6 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %45 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim1 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim1 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
where out dims are: [dim0 (size 2)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 2), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
   lane=16 -> (16)
where out dims are: [dim1 (size 32)]
[linear_layout]: checkInvariants:
 - lane is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim1 (size 32), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
where out dims are: [dim1 (size 32), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
   register=16 -> (16)
   register=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 2)
   register=2 -> (0, 4)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (16, 0)
 - warp=1 -> (32, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 2)
   register=2 -> (0, 4)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 1)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 2)
   register=2 -> (0, 4)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 1)
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (2, 0)
   register=2 -> (4, 0)
   register=4 -> (8, 0)
   register=8 -> (16, 0)
   register=16 -> (32, 0)
   register=32 -> (64, 0)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (1, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 1)]
%46 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %45 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %44 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %43 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (2)
   register=2 -> (4)
   register=4 -> (8)
   register=8 -> (16)
   register=16 -> (32)
   register=32 -> (64)
 - lane=1 -> (0)
   lane=2 -> (0)
   lane=4 -> (0)
   lane=8 -> (0)
   lane=16 -> (0)
 - warp=1 -> (0)
   warp=2 -> (1)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (2)
   register=2 -> (4)
   register=4 -> (8)
   register=8 -> (16)
   register=16 -> (32)
   register=32 -> (64)
 - lane=1 -> (0)
   lane=2 -> (0)
   lane=4 -> (0)
   lane=8 -> (0)
   lane=16 -> (0)
 - warp=1 -> (0)
   warp=2 -> (1)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
%4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}> %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %8 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}> %7 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %12 = arith.addi %8, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}> %11 = arith.addi %7, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %30 = tt.expand_dims %12 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %29 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %50 = arith.muli %30, %48 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %49 = arith.muli %29, %47 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %54 = tt.addptr %52, %50 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %53 = tt.addptr %51, %49 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %56 = tt.broadcast %54 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %55 = tt.broadcast %53 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %64 = tt.broadcast %61 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 1048576
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 0
[tritongpu-remove-layout-conversions]:   remat convert op %65 = ttg.convert_layout %64 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %64 = tt.broadcast %61 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %61 = arith.cmpi slt, %30, %60 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %60 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %30 = tt.expand_dims %12 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %29 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %61 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %60 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %63 = arith.cmpi slt, %30, %61 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %62 = arith.cmpi slt, %29, %60 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %67 = tt.broadcast %63 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %66 = tt.broadcast %62 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %30 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}> %29 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %60 = tt.addptr %56, %59 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 1048576
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 768
[tritongpu-remove-layout-conversions]:   remat convert op %72 = ttg.convert_layout %60 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %60 = tt.addptr %56, %59 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %59 = tt.broadcast %40 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %40 = tt.expand_dims %25 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %25 = arith.addi %21, %17 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %17 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %21 = tt.splat %15 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %56 = tt.broadcast %54 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %54 = tt.addptr %52, %50 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %50 = arith.muli %30, %48 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %48 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %30 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %11 = arith.addi %7, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %7 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]:     %52 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim1 (size 1), dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
where out dims are: [dim1 (size 16)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
where out dims are: [dim1 (size 16)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
where out dims are: [dim0 (size 2)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (0, 1)
where out dims are: [dim1 (size 16), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
where out dims are: [dim1 (size 4), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
   register=4 -> (4)
   register=8 -> (8)
where out dims are: [dim0 (size 16)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 0)
   register=2 -> (0, 0)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 0)
   register=2 -> (0, 0)
   register=4 -> (0, 8)
   register=8 -> (0, 16)
   register=16 -> (0, 32)
   register=32 -> (0, 64)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 0)
   register=2 -> (0, 0)
   register=4 -> (8, 0)
   register=8 -> (16, 0)
   register=16 -> (32, 0)
   register=32 -> (64, 0)
 - lane=1 -> (0, 0)
   lane=2 -> (0, 0)
   lane=4 -> (0, 0)
   lane=8 -> (0, 0)
   lane=16 -> (1, 0)
 - warp=1 -> (2, 0)
   warp=2 -> (4, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128), dim1 (size 1)]
%53 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %52 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %49 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %48 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - warp=1 -> (1)
   warp=2 -> (2)
where out dims are: [dim0 (size 4)]
[linear_layout]: checkInvariants:
 - warp=1 -> (0, 1)
   warp=2 -> (0, 2)
where out dims are: [dim1 (size 1), dim0 (size 4)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
where out dims are: [dim1 (size 16)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
   lane=2 -> (2)
   lane=4 -> (4)
   lane=8 -> (8)
where out dims are: [dim1 (size 16)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1)
where out dims are: [dim0 (size 2)]
[linear_layout]: checkInvariants:
 - lane=1 -> (1, 0)
   lane=2 -> (2, 0)
   lane=4 -> (4, 0)
   lane=8 -> (8, 0)
   lane=16 -> (0, 1)
where out dims are: [dim1 (size 16), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
where out dims are: [dim1 (size 4)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
where out dims are: [dim1 (size 4), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
where out dims are: [dim1 (size 64), dim0 (size 2)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 1)]
[linear_layout]: checkInvariants:
 - block is a size 1 dimension
where out dims are: [dim1 (size 1), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register is a size 1 dimension
where out dims are: [dim1 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 1)
 - warp=1 -> (0, 2)
   warp=2 -> (0, 4)
where out dims are: [dim1 (size 64), dim0 (size 8)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 0)
where out dims are: [dim1 (size 64), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (1, 0)
   register=2 -> (2, 0)
 - lane=1 -> (4, 0)
   lane=2 -> (8, 0)
   lane=4 -> (16, 0)
   lane=8 -> (32, 0)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim1 (size 64), dim0 (size 1)]
[linear_layout]: checkInvariants:
 - register=1 -> (0, 1)
   register=2 -> (0, 2)
 - lane=1 -> (0, 4)
   lane=2 -> (0, 8)
   lane=4 -> (0, 16)
   lane=8 -> (0, 32)
   lane=16 -> (0, 0)
 - warp=1 -> (0, 0)
   warp=2 -> (0, 0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 1), dim1 (size 64)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - dim0 is a size 1 dimension
 - dim1=1 -> (1)
   dim1=2 -> (2)
   dim1=4 -> (4)
   dim1=8 -> (8)
   dim1=16 -> (16)
   dim1=32 -> (32)
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
 - lane=1 -> (4)
   lane=2 -> (8)
   lane=4 -> (16)
   lane=8 -> (32)
   lane=16 -> (0)
 - warp=1 -> (0)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
[linear_layout]: checkInvariants:
 - register=1 -> (1)
   register=2 -> (2)
 - lane=1 -> (4)
   lane=2 -> (8)
   lane=4 -> (16)
   lane=8 -> (32)
   lane=16 -> (0)
 - warp=1 -> (0)
   warp=2 -> (0)
 - block is a size 1 dimension
where out dims are: [dim0 (size 64)]
%18 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}> %17 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue [linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 1)]
[linear_layout]: checkInvariants:
 - dim0=1 -> (1)
   dim0=2 -> (2)
   dim0=4 -> (4)
   dim0=8 -> (8)
   dim0=16 -> (16)
   dim0=32 -> (32)
   dim0=64 -> (64)
 - dim1 is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (0)
   register=2 -> (0)
   register=4 -> (8)
   register=8 -> (16)
   register=16 -> (32)
   register=32 -> (64)
 - lane=1 -> (0)
   lane=2 -> (0)
   lane=4 -> (0)
   lane=8 -> (0)
   lane=16 -> (1)
 - warp=1 -> (2)
   warp=2 -> (4)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
[linear_layout]: checkInvariants:
 - register=1 -> (8)
   register=2 -> (16)
   register=4 -> (32)
   register=8 -> (64)
 - lane=1 -> (0)
   lane=2 -> (0)
   lane=4 -> (0)
   lane=8 -> (0)
   lane=16 -> (1)
 - warp=1 -> (2)
   warp=2 -> (4)
 - block is a size 1 dimension
where out dims are: [dim0 (size 128)]
%4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}> %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %24 = tt.splat %16 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}> %23 = tt.splat %16 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %29 = arith.addi %24, %19 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}> %28 = arith.addi %23, %18 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %45 = tt.expand_dims %29 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %44 = tt.expand_dims %28 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %67 = tt.broadcast %45 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %66 = tt.broadcast %44 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %9 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}> %8 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %14 = arith.addi %9, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> encoding #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}> %13 = arith.addi %8, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
[tritongpu-remove-layout-conversions]: addRematValue %37 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %36 = tt.expand_dims %13 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %60 = arith.muli %37, %57 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %59 = arith.muli %36, %56 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %66 = tt.addptr %63, %60 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %65 = tt.addptr %62, %59 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %69 = tt.broadcast %66 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %68 = tt.broadcast %65 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %75 = tt.addptr %69, %73 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %74 = tt.addptr %68, %72 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %55 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
  %90 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %91 = tt.addptr %44, %90 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %92 = tt.load %91 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %93 = arith.muli %arg9, %arg7 : i32
  %94 = tt.splat %93 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %95 = tt.splat %93 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %96 = tt.addptr %53, %94 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %97 = tt.addptr %54, %95 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %98 = tt.load %96 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %99 = tt.load %97 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %100 = tt.broadcast %92 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %101 = tt.broadcast %98 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %102 = tt.broadcast %99 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %103 = ttg.convert_layout %102 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %104 = arith.mulf %100, %101 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %105 = arith.addf %arg10, %104 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  scf.yield %105 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
} encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   getRematerializableSlice failed
[tritongpu-remove-layout-conversions]: addRematValue %55 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
  %90 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %91 = tt.addptr %44, %90 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %92 = tt.load %91 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %93 = arith.muli %arg9, %arg7 : i32
  %94 = tt.splat %93 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %95 = tt.splat %93 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %96 = tt.addptr %53, %94 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %97 = tt.addptr %54, %95 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %98 = tt.load %96 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %99 = tt.load %97 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %100 = tt.broadcast %92 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %101 = tt.broadcast %98 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %102 = tt.broadcast %99 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
  %103 = ttg.convert_layout %102 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %104 = arith.mulf %100, %101 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %105 = arith.addf %arg10, %104 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  scf.yield %105 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
} encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %88 = ttg.convert_layout %55 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: check backward remat with source %86 = arith.andi %82, %85 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
[tritongpu-remove-layout-conversions]:   convert layout cost: 1048576
[tritongpu-remove-layout-conversions]:   rematerialisation cost: 0
[tritongpu-remove-layout-conversions]:   remat convert op %89 = ttg.convert_layout %86 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %86 = arith.andi %82, %85 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %85 = tt.broadcast %81 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %81 = arith.cmpi slt, %48, %80 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %80 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %48 = tt.expand_dims %31 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %47 = tt.expand_dims %30 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %82 = tt.broadcast %78 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %78 = arith.cmpi slt, %37, %76 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %76 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %37 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]:     %36 = tt.expand_dims %13 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %81 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %80 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %77 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %76 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %84 = arith.cmpi slt, %48, %82 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %83 = arith.cmpi slt, %47, %81 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %89 = tt.broadcast %84 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %88 = tt.broadcast %83 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %48 = tt.expand_dims %30 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %47 = tt.expand_dims %30 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %81 = arith.cmpi slt, %37, %78 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %80 = arith.cmpi slt, %36, %77 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %88 = tt.broadcast %81 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %87 = tt.broadcast %80 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %94 = arith.andi %88, %92 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %93 = arith.andi %87, %91 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %37 = tt.expand_dims %13 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %36 = tt.expand_dims %13 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: Module after backward remat:
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.muli %1, %c128_i32 : i32
    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
    %5 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %6 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
    %7 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %8 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %9 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
    %10 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %11 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
    %12 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %13 = arith.addi %8, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %14 = arith.addi %9, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
    %15 = arith.addi %10, %5 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %16 = arith.addi %11, %6 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>
    %17 = arith.addi %12, %7 : tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %18 = arith.muli %0, %c64_i32 : i32
    %19 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %20 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %21 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
    %22 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
    %23 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %24 = tt.splat %18 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %25 = tt.splat %18 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %26 = tt.splat %18 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
    %27 = tt.splat %18 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
    %28 = tt.splat %18 : i32 -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %29 = arith.addi %24, %19 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %30 = arith.addi %25, %20 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %31 = arith.addi %26, %21 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>
    %32 = arith.addi %27, %22 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>
    %33 = arith.addi %28, %23 : tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>
    %34 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %35 = tt.expand_dims %13 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %36 = tt.expand_dims %13 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %37 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %38 = tt.expand_dims %15 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %39 = tt.expand_dims %16 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>
    %40 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %41 = arith.muli %38, %40 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %42 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %43 = tt.addptr %42, %41 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %44 = tt.expand_dims %29 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %45 = tt.expand_dims %30 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %46 = tt.expand_dims %30 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %47 = tt.expand_dims %31 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %48 = tt.expand_dims %32 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>
    %49 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %50 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %51 = tt.addptr %49, %44 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %52 = tt.addptr %50, %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %53 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
      %91 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %92 = tt.addptr %43, %91 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %93 = tt.load %92 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %94 = arith.muli %arg9, %arg7 : i32
      %95 = tt.splat %94 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %96 = tt.splat %94 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %97 = tt.addptr %51, %95 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %98 = tt.addptr %52, %96 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %99 = tt.load %97 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %100 = tt.load %98 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %101 = tt.broadcast %93 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %102 = tt.broadcast %99 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %103 = tt.broadcast %100 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
      %104 = arith.mulf %101, %102 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %105 = arith.addf %arg10, %104 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      scf.yield %105 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    }
    %54 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %55 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %56 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %57 = arith.muli %36, %54 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %58 = arith.muli %37, %55 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %59 = arith.muli %38, %56 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %60 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %61 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %62 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %63 = tt.addptr %60, %57 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %64 = tt.addptr %61, %58 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %65 = tt.addptr %62, %59 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %66 = tt.broadcast %63 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %67 = tt.broadcast %64 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %68 = tt.broadcast %65 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %69 = tt.broadcast %46 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %70 = tt.broadcast %47 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %71 = tt.addptr %66, %69 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %72 = tt.addptr %67, %70 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %73 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %74 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %75 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %76 = arith.cmpi slt, %36, %73 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %77 = arith.cmpi slt, %37, %74 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %78 = arith.cmpi slt, %38, %75 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %79 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %80 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %81 = arith.cmpi slt, %46, %79 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %82 = arith.cmpi slt, %47, %80 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %83 = tt.broadcast %76 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %84 = tt.broadcast %77 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %85 = tt.broadcast %78 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %86 = tt.broadcast %81 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %87 = tt.broadcast %82 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %88 = arith.andi %83, %86 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %89 = arith.andi %84, %87 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>
    %90 = ttg.convert_layout %53 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.store %71, %90, %88 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.return
  }
}

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x56020434b210) {
  "tt.return"() : () -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x560204387c60) {
  "tt.store"(%76, %95, %93) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392f90) {
  %95 = "ttg.convert_layout"(%58) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x560204392dd0) {
  %94 = "arith.andi"(%89, %92) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'arith.andi'(0x560204392dd0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5602043a6340) {
  %93 = "arith.andi"(%88, %91) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392d40) {
  %92 = "tt.broadcast"(%87) : (tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x560204392d40)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a91c0) {
  %91 = "tt.broadcast"(%86) : (tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392c20) {
  %90 = "tt.broadcast"(%83) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x560204392c20)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a45d0) {
  %89 = "tt.broadcast"(%82) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x5602043a45d0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043acf70) {
  %88 = "tt.broadcast"(%81) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x560204392550) {
  %87 = "arith.cmpi"(%52, %85) <{predicate = 2 : i64}> : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'arith.cmpi'(0x560204392550)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602042ebdb0) {
  %86 = "arith.cmpi"(%51, %84) <{predicate = 2 : i64}> : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043924c0) {
  %85 = "tt.splat"(%arg5) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x5602043924c0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a9080) {
  %84 = "tt.splat"(%arg5) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x560204392410) {
  %83 = "arith.cmpi"(%43, %80) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'arith.cmpi'(0x560204392410)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602043a4660) {
  %82 = "arith.cmpi"(%42, %79) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'arith.cmpi'(0x5602043a4660)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602043ad090) {
  %81 = "arith.cmpi"(%41, %78) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204392380) {
  %80 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x560204392380)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043aafe0) {
  %79 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x5602043aafe0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a66c0) {
  %78 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043922d0) {
  %77 = "tt.addptr"(%72, %75) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.addptr'(0x5602043922d0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043aac60) {
  %76 = "tt.addptr"(%71, %74) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392240) {
  %75 = "tt.broadcast"(%52) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x560204392240)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020439ab60) {
  %74 = "tt.broadcast"(%51) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204392120) {
  %73 = "tt.broadcast"(%70) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x560204392120)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a9c20) {
  %72 = "tt.broadcast"(%69) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x5602043a9c20)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020439abf0) {
  %71 = "tt.broadcast"(%68) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x560204392070) {
  %70 = "tt.addptr"(%67, %64) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.addptr'(0x560204392070)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043ac710) {
  %69 = "tt.addptr"(%66, %63) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.addptr'(0x5602043ac710)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a9110) {
  %68 = "tt.addptr"(%65, %62) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204391fe0) {
  %67 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x560204391fe0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a4ab0) {
  %66 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x5602043a4ab0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a7380) {
  %65 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204391790) {
  %64 = "arith.muli"(%43, %61) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'arith.muli'(0x560204391790)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5602043a9ef0) {
  %63 = "arith.muli"(%42, %60) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'arith.muli'(0x5602043a9ef0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5602043b3510) {
  %62 = "arith.muli"(%41, %59) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204391700) {
  %61 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x560204391700)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043ac9e0) {
  %60 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x5602043ac9e0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a7410) {
  %59 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x560204391f50) {
  "scf.yield"(%88) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5602043852e0) {
  %88 = "arith.addf"(%arg10, %87) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x560204385230) {
  %87 = "arith.mulf"(%84, %85) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020438f060) {
  %86 = "tt.broadcast"(%83) : (tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.broadcast'(0x56020438f060)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a6d10) {
  %85 = "tt.broadcast"(%82) : (tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204390f00) {
  %84 = "tt.broadcast"(%76) : (tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5602043909a0) {
  %83 = "tt.load"(%81) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.load'(0x5602043909a0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020439a3a0) {
  %82 = "tt.load"(%80) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x560204390760) {
  %81 = "tt.addptr"(%57, %79) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.addptr'(0x560204390760)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a4940) {
  %80 = "tt.addptr"(%56, %78) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204390060) {
  %79 = "tt.splat"(%77) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x560204390060)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a3e40) {
  %78 = "tt.splat"(%77) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438ff50) {
  %77 = "arith.muli"(%arg9, %arg7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020438f9f0) {
  %76 = "tt.load"(%75) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438f330) {
  %75 = "tt.addptr"(%48, %74) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x560204398500) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438f240) {
  %74 = "tt.splat"(%arg9) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438ee80) {
  %57 = "tt.addptr"(%55, %52) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.addptr'(0x56020438ee80)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a3f70) {
  %56 = "tt.addptr"(%54, %49) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438ed90) {
  %55 = "tt.splat"(%arg1) : (!tt.ptr<f32>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.splat'(0x56020438ed90)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439aed0) {
  %54 = "tt.splat"(%arg1) : (!tt.ptr<f32>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x56020438e730) {
  %53 = "tt.expand_dims"(%37) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>>

  ** Erase   : 'tt.expand_dims'(0x56020438e730)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a5d70) {
  %52 = "tt.expand_dims"(%36) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.expand_dims'(0x5602043a5d70)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a59d0) {
  %51 = "tt.expand_dims"(%35) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602042ebd20) {
  %50 = "tt.expand_dims"(%35) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.expand_dims'(0x5602042ebd20)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a24a0) {
  %49 = "tt.expand_dims"(%34) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438e0b0) {
  %48 = "tt.addptr"(%47, %46) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438dfc0) {
  %47 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438da30) {
  %46 = "arith.muli"(%43, %45) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438d520) {
  %45 = "tt.splat"(%arg6) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043875b0) {
  %44 = "tt.expand_dims"(%21) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>>

  ** Erase   : 'tt.expand_dims'(0x5602043875b0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x56020439b5e0) {
  %43 = "tt.expand_dims"(%20) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a44a0) {
  %42 = "tt.expand_dims"(%19) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.expand_dims'(0x5602043a44a0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a8ff0) {
  %41 = "tt.expand_dims"(%18) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043ad000) {
  %40 = "tt.expand_dims"(%18) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

  ** Erase   : 'tt.expand_dims'(0x5602043ad000)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a9dc0) {
  %39 = "tt.expand_dims"(%19) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

  ** Erase   : 'tt.expand_dims'(0x5602043a9dc0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x56020438a750) {
  %38 = "arith.addi"(%33, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>, tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

  ** Erase   : 'arith.addi'(0x56020438a750)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204391e50) {
  %37 = "arith.addi"(%32, %27) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>

  ** Erase   : 'arith.addi'(0x560204391e50)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204398870) {
  %36 = "arith.addi"(%31, %26) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>

  ** Erase   : 'arith.addi'(0x560204398870)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043b3420) {
  %35 = "arith.addi"(%30, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204399660) {
  %34 = "arith.addi"(%29, %24) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438a660) {
  %33 = "tt.splat"(%23) : (i32) -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

  ** Erase   : 'tt.splat'(0x56020438a660)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204390870) {
  %32 = "tt.splat"(%23) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>

  ** Erase   : 'tt.splat'(0x560204390870)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a5ce0) {
  %31 = "tt.splat"(%23) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>

  ** Erase   : 'tt.splat'(0x5602043a5ce0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439a970) {
  %30 = "tt.splat"(%23) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a4090) {
  %29 = "tt.splat"(%23) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020438a110) {
  %28 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

  ** Erase   : 'tt.make_range'(0x56020438a110)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x560204392a10) {
  %27 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>}>>

  ** Erase   : 'tt.make_range'(0x560204392a10)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x560204386410) {
  %26 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>

  ** Erase   : 'tt.make_range'(0x560204386410)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a2b80) {
  %25 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043929a0) {
  %24 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204389280) {
  %23 = "arith.muli"(%5, %1) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204388cf0) {
  %22 = "arith.addi"(%17, %12) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>, tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

  ** Erase   : 'arith.addi'(0x560204388cf0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043918c0) {
  %21 = "arith.addi"(%16, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>

  ** Erase   : 'arith.addi'(0x5602043918c0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043995b0) {
  %20 = "arith.addi"(%15, %10) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043ab3f0) {
  %19 = "arith.addi"(%14, %9) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>

  ** Erase   : 'arith.addi'(0x5602043ab3f0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043af430) {
  %18 = "arith.addi"(%13, %8) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x560204381bd0) {
  %17 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

  ** Erase   : 'tt.splat'(0x560204381bd0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439da70) {
  %16 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>

  ** Erase   : 'tt.splat'(0x56020439da70)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439ddd0) {
  %15 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a9d30) {
  %14 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>

  ** Erase   : 'tt.splat'(0x5602043a9d30)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439aa00) {
  %13 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x560204388bc0) {
  %12 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>

  ** Erase   : 'tt.make_range'(0x560204388bc0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020439a890) {
  %11 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>}>>

  ** Erase   : 'tt.make_range'(0x56020439a890)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020439e570) {
  %10 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a3650) {
  %9 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>}>>

  ** Erase   : 'tt.make_range'(0x5602043a3650)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a4b40) {
  %8 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204388630) {
  %7 = "arith.muli"(%6, %0) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x560204355130) {
  %6 = "tt.get_program_id"() <{axis = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5602043540a0) {
  %5 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5602043a54e0) {
  %4 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>}> : () -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386a70) {
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386ff0) {
  %1 = "arith.constant"() <{value = 64 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x560204392690) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204385500) {
  %0 = "arith.constant"() <{value = 128 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x56020434b210) {
  "tt.return"() : () -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x560204387c60) {
  "tt.store"(%38, %46, %45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392f90) {
  %46 = "ttg.convert_layout"(%31) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5602043a6340) {
  %45 = "arith.andi"(%43, %44) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a91c0) {
  %44 = "tt.broadcast"(%42) : (tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043acf70) {
  %43 = "tt.broadcast"(%40) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602042ebdb0) {
  %42 = "arith.cmpi"(%28, %41) <{predicate = 2 : i64}> : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a9080) {
  %41 = "tt.splat"(%arg5) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602043ad090) {
  %40 = "arith.cmpi"(%21, %39) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a66c0) {
  %39 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043aac60) {
  %38 = "tt.addptr"(%36, %37) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020439ab60) {
  %37 = "tt.broadcast"(%28) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020439abf0) {
  %36 = "tt.broadcast"(%35) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a9110) {
  %35 = "tt.addptr"(%34, %33) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a7380) {
  %34 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5602043b3510) {
  %33 = "arith.muli"(%21, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a7410) {
  %32 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x560204391f50) {
  "scf.yield"(%57) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5602043852e0) {
  %57 = "arith.addf"(%arg10, %56) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x560204385230) {
  %56 = "arith.mulf"(%54, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a6d10) {
  %55 = "tt.broadcast"(%53) : (tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204390f00) {
  %54 = "tt.broadcast"(%49) : (tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020439a3a0) {
  %53 = "tt.load"(%52) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a4940) {
  %52 = "tt.addptr"(%30, %51) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a3e40) {
  %51 = "tt.splat"(%50) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438ff50) {
  %50 = "arith.muli"(%arg9, %arg7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020438f9f0) {
  %49 = "tt.load"(%48) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438f330) {
  %48 = "tt.addptr"(%26, %47) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x560204398500) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438f240) {
  %47 = "tt.splat"(%arg9) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a3f70) {
  %30 = "tt.addptr"(%29, %27) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439aed0) {
  %29 = "tt.splat"(%arg1) : (!tt.ptr<f32>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a59d0) {
  %28 = "tt.expand_dims"(%20) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a24a0) {
  %27 = "tt.expand_dims"(%19) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438e0b0) {
  %26 = "tt.addptr"(%25, %24) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438dfc0) {
  %25 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438da30) {
  %24 = "arith.muli"(%22, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438d520) {
  %23 = "tt.splat"(%arg6) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x56020439b5e0) {
  %22 = "tt.expand_dims"(%13) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a8ff0) {
  %21 = "tt.expand_dims"(%12) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043b3420) {
  %20 = "arith.addi"(%18, %16) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204399660) {
  %19 = "arith.addi"(%17, %15) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439a970) {
  %18 = "tt.splat"(%14) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a4090) {
  %17 = "tt.splat"(%14) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a2b80) {
  %16 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043929a0) {
  %15 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204389280) {
  %14 = "arith.muli"(%5, %1) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043995b0) {
  %13 = "arith.addi"(%11, %9) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043af430) {
  %12 = "arith.addi"(%10, %8) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439ddd0) {
  %11 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439aa00) {
  %10 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020439e570) {
  %9 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a4b40) {
  %8 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204388630) {
  %7 = "arith.muli"(%6, %0) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x560204355130) {
  %6 = "tt.get_program_id"() <{axis = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5602043540a0) {
  %5 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5602043a54e0) {
  %4 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>}> : () -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386a70) {
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386ff0) {
  %1 = "arith.constant"() <{value = 64 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x560204392690) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204385500) {
  %0 = "arith.constant"() <{value = 128 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//
[tritongpu-remove-layout-conversions]: Module after canonicalizing:
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.muli %1, %c128_i32 : i32
    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %5 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %6 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %7 = arith.addi %5, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %8 = arith.addi %6, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %9 = arith.muli %0, %c64_i32 : i32
    %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %11 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %12 = tt.splat %9 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %13 = tt.splat %9 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %14 = arith.addi %12, %10 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %15 = arith.addi %13, %11 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %16 = tt.expand_dims %7 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %17 = tt.expand_dims %8 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %18 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %19 = arith.muli %17, %18 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %20 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %21 = tt.addptr %20, %19 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %22 = tt.expand_dims %14 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %23 = tt.expand_dims %15 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %24 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %25 = tt.addptr %24, %22 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %26 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
      %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %43 = tt.addptr %21, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %45 = arith.muli %arg9, %arg7 : i32
      %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %47 = tt.addptr %25, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %51 = arith.mulf %49, %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %52 = arith.addf %arg10, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      scf.yield %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    }
    %27 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %28 = arith.muli %16, %27 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %29 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %30 = tt.addptr %29, %28 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %31 = tt.broadcast %30 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %32 = tt.broadcast %23 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %33 = tt.addptr %31, %32 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %34 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %35 = arith.cmpi slt, %16, %34 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %36 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %37 = arith.cmpi slt, %23, %36 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %38 = tt.broadcast %35 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %39 = tt.broadcast %37 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %40 = arith.andi %38, %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %41 = ttg.convert_layout %26 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.store %33, %41, %40 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.return
  }
}
[tritongpu-remove-layout-conversions]: addRematValue %26 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
  %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %43 = tt.addptr %21, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %45 = arith.muli %arg9, %arg7 : i32
  %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %47 = tt.addptr %25, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %51 = arith.mulf %49, %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %52 = arith.addf %arg10, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  scf.yield %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
} encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %41 = ttg.convert_layout %26 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %26 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
  %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %43 = tt.addptr %21, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %45 = arith.muli %arg9, %arg7 : i32
  %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %47 = tt.addptr %25, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %51 = arith.mulf %49, %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %52 = arith.addf %arg10, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  scf.yield %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
} encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %41 = ttg.convert_layout %26 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: addRematValue %26 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
  %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %43 = tt.addptr %21, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %45 = arith.muli %arg9, %arg7 : i32
  %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %47 = tt.addptr %25, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %51 = arith.mulf %49, %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  %52 = arith.addf %arg10, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
  scf.yield %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
} encoding #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}> %41 = ttg.convert_layout %26 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
[tritongpu-remove-layout-conversions]: Module after hoisting converts:
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.muli %1, %c128_i32 : i32
    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %5 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %6 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %7 = arith.addi %5, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %8 = arith.addi %6, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %9 = arith.muli %0, %c64_i32 : i32
    %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %11 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %12 = tt.splat %9 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %13 = tt.splat %9 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %14 = arith.addi %12, %10 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %15 = arith.addi %13, %11 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %16 = tt.expand_dims %7 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %17 = tt.expand_dims %8 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %18 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %19 = arith.muli %17, %18 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %20 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %21 = tt.addptr %20, %19 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %22 = tt.expand_dims %14 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %23 = tt.expand_dims %15 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %24 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %25 = tt.addptr %24, %22 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %26 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
      %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %43 = tt.addptr %21, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %45 = arith.muli %arg9, %arg7 : i32
      %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %47 = tt.addptr %25, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %51 = arith.mulf %49, %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %52 = arith.addf %arg10, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      scf.yield %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    }
    %27 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %28 = arith.muli %16, %27 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %29 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %30 = tt.addptr %29, %28 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %31 = tt.broadcast %30 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %32 = tt.broadcast %23 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %33 = tt.addptr %31, %32 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %34 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %35 = arith.cmpi slt, %16, %34 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %36 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %37 = arith.cmpi slt, %23, %36 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %38 = tt.broadcast %35 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %39 = tt.broadcast %37 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %40 = arith.andi %38, %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %41 = ttg.convert_layout %26 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.store %33, %41, %40 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.return
  }
}

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x56020434b210) {
  "tt.return"() : () -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x560204387c60) {
  "tt.store"(%38, %46, %45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'ttg.convert_layout'(0x560204392f90) {
  %46 = "ttg.convert_layout"(%31) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>


  * Pattern mlir::triton::gpu::CanonicalizeConvertFromConvert : 'ttg.convert_layout -> ()' {
Trying to match "mlir::triton::gpu::CanonicalizeConvertFromConvert"
"mlir::triton::gpu::CanonicalizeConvertFromConvert" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5602043a6340) {
  %45 = "arith.andi"(%43, %44) : (tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a91c0) {
  %44 = "tt.broadcast"(%42) : (tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043acf70) {
  %43 = "tt.broadcast"(%40) : (tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602042ebdb0) {
  %42 = "arith.cmpi"(%28, %41) <{predicate = 2 : i64}> : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a9080) {
  %41 = "tt.splat"(%arg5) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5602043ad090) {
  %40 = "arith.cmpi"(%21, %39) <{predicate = 2 : i64}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a66c0) {
  %39 = "tt.splat"(%arg3) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043aac60) {
  %38 = "tt.addptr"(%36, %37) : (tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020439ab60) {
  %37 = "tt.broadcast"(%28) : (tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x56020439abf0) {
  %36 = "tt.broadcast"(%35) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a9110) {
  %35 = "tt.addptr"(%34, %33) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a7380) {
  %34 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5602043b3510) {
  %33 = "arith.muli"(%21, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a7410) {
  %32 = "tt.splat"(%arg8) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x560204391f50) {
  "scf.yield"(%57) : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> ()

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5602043852e0) {
  %57 = "arith.addf"(%arg10, %56) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x560204385230) {
  %56 = "arith.mulf"(%54, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5602043a6d10) {
  %55 = "tt.broadcast"(%53) : (tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x560204390f00) {
  %54 = "tt.broadcast"(%49) : (tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020439a3a0) {
  %53 = "tt.load"(%52) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a4940) {
  %52 = "tt.addptr"(%30, %51) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a3e40) {
  %51 = "tt.splat"(%50) : (i32) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438ff50) {
  %50 = "arith.muli"(%arg9, %arg7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x56020438f9f0) {
  %49 = "tt.load"(%48) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438f330) {
  %48 = "tt.addptr"(%26, %47) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x560204398500) {

  * Pattern mlir::{anonymous}::ForOpDeadArgElimination : 'scf.for -> ()' {
Trying to match "mlir::{anonymous}::ForOpDeadArgElimination"
"mlir::{anonymous}::ForOpDeadArgElimination" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ForOpIterArgsFolder : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::ForOpIterArgsFolder"
"(anonymous namespace)::ForOpIterArgsFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::SimplifyTrivialLoops : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::SimplifyTrivialLoops"
"(anonymous namespace)::SimplifyTrivialLoops" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ForOpTensorCastFolder : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::ForOpTensorCastFolder"
"(anonymous namespace)::ForOpTensorCastFolder" result 0
  } -> failure : pattern failed to match
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438f240) {
  %47 = "tt.splat"(%arg9) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5602043a3f70) {
  %30 = "tt.addptr"(%29, %27) : (tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439aed0) {
  %29 = "tt.splat"(%arg1) : (!tt.ptr<f32>) -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a59d0) {
  %28 = "tt.expand_dims"(%20) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a24a0) {
  %27 = "tt.expand_dims"(%19) <{axis = 0 : i32}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x56020438e0b0) {
  %26 = "tt.addptr"(%25, %24) : (tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438dfc0) {
  %25 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x56020438da30) {
  %24 = "arith.muli"(%22, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020438d520) {
  %23 = "tt.splat"(%arg6) : (i32) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x56020439b5e0) {
  %22 = "tt.expand_dims"(%13) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5602043a8ff0) {
  %21 = "tt.expand_dims"(%12) <{axis = 1 : i32}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043b3420) {
  %20 = "arith.addi"(%18, %16) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x560204399660) {
  %19 = "arith.addi"(%17, %15) <{overflowFlags = #arith.overflow<none>}> : (tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439a970) {
  %18 = "tt.splat"(%14) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5602043a4090) {
  %17 = "tt.splat"(%14) : (i32) -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a2b80) {
  %16 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043929a0) {
  %15 = "tt.make_range"() <{end = 64 : i32, start = 0 : i32}> : () -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204389280) {
  %14 = "arith.muli"(%5, %1) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043995b0) {
  %13 = "arith.addi"(%11, %9) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5602043af430) {
  %12 = "arith.addi"(%10, %8) <{overflowFlags = #arith.overflow<none>}> : (tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439ddd0) {
  %11 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x56020439aa00) {
  %10 = "tt.splat"(%7) : (i32) -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x56020439e570) {
  %9 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5602043a4b40) {
  %8 = "tt.make_range"() <{end = 128 : i32, start = 0 : i32}> : () -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x560204388630) {
  %7 = "arith.muli"(%6, %0) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x560204355130) {
  %6 = "tt.get_program_id"() <{axis = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5602043540a0) {
  %5 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5602043a54e0) {
  %4 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>}> : () -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386a70) {
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204386ff0) {
  %1 = "arith.constant"() <{value = 64 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x560204392690) {
} -> failure : all patterns failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x560204385500) {
  %0 = "arith.constant"() <{value = 128 : i32}> : () -> i32

} -> failure : all patterns failed to match
//===-------------------------------------------===//
[tritongpu-remove-layout-conversions]: Module after final cleanups:
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.muli %1, %c128_i32 : i32
    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %5 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %6 = tt.splat %2 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %7 = arith.addi %5, %3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %8 = arith.addi %6, %4 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %9 = arith.muli %0, %c64_i32 : i32
    %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %11 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %12 = tt.splat %9 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %13 = tt.splat %9 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %14 = arith.addi %12, %10 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %15 = arith.addi %13, %11 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>>
    %16 = tt.expand_dims %7 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %17 = tt.expand_dims %8 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %18 = tt.splat %arg6 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %19 = arith.muli %17, %18 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %20 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %21 = tt.addptr %20, %19 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %22 = tt.expand_dims %14 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %23 = tt.expand_dims %15 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>}>> -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %24 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %25 = tt.addptr %24, %22 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %26 = scf.for %arg9 = %c0_i32 to %arg4 step %c1_i32 iter_args(%arg10 = %cst) -> (tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>)  : i32 {
      %42 = tt.splat %arg9 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %43 = tt.addptr %21, %42 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %44 = tt.load %43 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %45 = arith.muli %arg9, %arg7 : i32
      %46 = tt.splat %45 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %47 = tt.addptr %25, %46 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %48 = tt.load %47 : tensor<1x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %49 = tt.broadcast %44 : tensor<128x1xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %50 = tt.broadcast %48 : tensor<1x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %51 = arith.mulf %49, %50 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      %52 = arith.addf %arg10, %51 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
      scf.yield %52 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>>
    }
    %27 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %28 = arith.muli %16, %27 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %29 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %30 = tt.addptr %29, %28 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %31 = tt.broadcast %30 : tensor<128x1x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %32 = tt.broadcast %23 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %33 = tt.addptr %31, %32 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>, tensor<128x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %34 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %35 = arith.cmpi slt, %16, %34 : tensor<128x1xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %36 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %37 = arith.cmpi slt, %23, %36 : tensor<1x64xi32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %38 = tt.broadcast %35 : tensor<128x1xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %39 = tt.broadcast %37 : tensor<1x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %40 = arith.andi %38, %39 : tensor<128x64xi1, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    %41 = ttg.convert_layout %26 : tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>> -> tensor<128x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.store %33, %41, %40 : tensor<128x64x!tt.ptr<f32>, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>
    tt.return
  }
}
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::TritonGPU_AttrTrait::Trait<mlir::TypeID::get<mlir::triton::gpu::TritonGPU_AttrTrait::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::DistributedEncodingTrait::Trait<mlir::TypeID::get<mlir::triton::gpu::DistributedEncodingTrait::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::gpu::LayoutEncodingTrait::Trait<mlir::TypeID::get<mlir::triton::gpu::LayoutEncodingTrait::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::StorageUserTrait::IsMutable<Empty>)
