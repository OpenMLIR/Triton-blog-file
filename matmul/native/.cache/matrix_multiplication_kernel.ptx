//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_100a
.address_size 64

	// .globl	matrix_multiplication_kernel // -- Begin function matrix_multiplication_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @matrix_multiplication_kernel
.visible .entry matrix_multiplication_kernel(
	.param .u64 .ptr .global .align 1 matrix_multiplication_kernel_param_0,
	.param .u64 .ptr .global .align 1 matrix_multiplication_kernel_param_1,
	.param .u64 .ptr .global .align 1 matrix_multiplication_kernel_param_2,
	.param .u32 matrix_multiplication_kernel_param_3,
	.param .u32 matrix_multiplication_kernel_param_4,
	.param .u32 matrix_multiplication_kernel_param_5,
	.param .u32 matrix_multiplication_kernel_param_6,
	.param .u32 matrix_multiplication_kernel_param_7,
	.param .u32 matrix_multiplication_kernel_param_8,
	.param .u64 .ptr .global .align 1 matrix_multiplication_kernel_param_9
)
.reqntid 128
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<660>;
	.reg .b64 	%rd<82>;
	.loc	1 6 0                           // matmul.py:6:0
$L__func_begin0:
	.loc	1 6 0                           // matmul.py:6:0

// %bb.0:
	ld.param.b32 	%r204, [matrix_multiplication_kernel_param_8];
	ld.param.b32 	%r201, [matrix_multiplication_kernel_param_5];
	ld.param.b32 	%r200, [matrix_multiplication_kernel_param_4];
	ld.param.b32 	%r199, [matrix_multiplication_kernel_param_3];
	ld.param.b64 	%rd9, [matrix_multiplication_kernel_param_2];
$L__tmp0:
	.loc	1 17 26                         // matmul.py:17:26
	mov.u32 	%r269, %ctaid.x;
	.loc	1 18 26                         // matmul.py:18:26
	mov.u32 	%r270, %ctaid.y;
	.loc	1 20 21                         // matmul.py:20:21
	shl.b32 	%r1, %r270, 7;
	.loc	1 20 49                         // matmul.py:20:49
	mov.u32 	%r2, %tid.x;
	and.b32 	%r3, %r2, 127;
	.loc	1 21 21                         // matmul.py:21:21
	shl.b32 	%r4, %r269, 6;
	.loc	1 30 19                         // matmul.py:30:19
	setp.lt.s32 	%p1, %r200, 1;
	mov.b32 	%r596, 0f00000000;
	mov.b32 	%r597, %r596;
	mov.b32 	%r598, %r596;
	mov.b32 	%r599, %r596;
	mov.b32 	%r600, %r596;
	mov.b32 	%r601, %r596;
	mov.b32 	%r602, %r596;
	mov.b32 	%r603, %r596;
	mov.b32 	%r604, %r596;
	mov.b32 	%r605, %r596;
	mov.b32 	%r606, %r596;
	mov.b32 	%r607, %r596;
	mov.b32 	%r608, %r596;
	mov.b32 	%r609, %r596;
	mov.b32 	%r610, %r596;
	mov.b32 	%r611, %r596;
	mov.b32 	%r612, %r596;
	mov.b32 	%r613, %r596;
	mov.b32 	%r614, %r596;
	mov.b32 	%r615, %r596;
	mov.b32 	%r616, %r596;
	mov.b32 	%r617, %r596;
	mov.b32 	%r618, %r596;
	mov.b32 	%r619, %r596;
	mov.b32 	%r620, %r596;
	mov.b32 	%r621, %r596;
	mov.b32 	%r622, %r596;
	mov.b32 	%r623, %r596;
	mov.b32 	%r624, %r596;
	mov.b32 	%r625, %r596;
	mov.b32 	%r626, %r596;
	mov.b32 	%r627, %r596;
	mov.b32 	%r628, %r596;
	mov.b32 	%r629, %r596;
	mov.b32 	%r630, %r596;
	mov.b32 	%r631, %r596;
	mov.b32 	%r632, %r596;
	mov.b32 	%r633, %r596;
	mov.b32 	%r634, %r596;
	mov.b32 	%r635, %r596;
	mov.b32 	%r636, %r596;
	mov.b32 	%r637, %r596;
	mov.b32 	%r638, %r596;
	mov.b32 	%r639, %r596;
	mov.b32 	%r640, %r596;
	mov.b32 	%r641, %r596;
	mov.b32 	%r642, %r596;
	mov.b32 	%r643, %r596;
	mov.b32 	%r644, %r596;
	mov.b32 	%r645, %r596;
	mov.b32 	%r646, %r596;
	mov.b32 	%r647, %r596;
	mov.b32 	%r648, %r596;
	mov.b32 	%r649, %r596;
	mov.b32 	%r650, %r596;
	mov.b32 	%r651, %r596;
	mov.b32 	%r652, %r596;
	mov.b32 	%r653, %r596;
	mov.b32 	%r654, %r596;
	mov.b32 	%r655, %r596;
	mov.b32 	%r656, %r596;
	mov.b32 	%r657, %r596;
	mov.b32 	%r658, %r596;
	mov.b32 	%r659, %r596;
	@%p1 bra 	$L__BB0_3;
// %bb.1:                               // %.lr.ph.preheader
	.loc	1 0 19                          // matmul.py:0:19
	ld.param.b32 	%r203, [matrix_multiplication_kernel_param_7];
	ld.param.b32 	%r202, [matrix_multiplication_kernel_param_6];
	ld.param.b64 	%rd8, [matrix_multiplication_kernel_param_0];
	ld.param.b64 	%rd10, [matrix_multiplication_kernel_param_1];
	mul.wide.s32 	%rd11, %r4, 4;
	add.s64 	%rd1, %rd10, %rd11;
	.loc	1 30 19                         // matmul.py:30:19
	cvt.u64.u32 	%rd81, %r200;
	add.s32 	%r336, %r1, %r3;
	mul.lo.s32 	%r337, %r202, %r336;
	mul.wide.s32 	%rd12, %r337, 4;
	add.s64 	%rd80, %rd8, %rd12;
	mov.b32 	%r596, 0f00000000;
	mov.b32 	%r531, 0;
	mov.b32 	%r597, %r596;
	mov.b32 	%r598, %r596;
	mov.b32 	%r599, %r596;
	mov.b32 	%r600, %r596;
	mov.b32 	%r601, %r596;
	mov.b32 	%r602, %r596;
	mov.b32 	%r603, %r596;
	mov.b32 	%r604, %r596;
	mov.b32 	%r605, %r596;
	mov.b32 	%r606, %r596;
	mov.b32 	%r607, %r596;
	mov.b32 	%r608, %r596;
	mov.b32 	%r609, %r596;
	mov.b32 	%r610, %r596;
	mov.b32 	%r611, %r596;
	mov.b32 	%r612, %r596;
	mov.b32 	%r613, %r596;
	mov.b32 	%r614, %r596;
	mov.b32 	%r615, %r596;
	mov.b32 	%r616, %r596;
	mov.b32 	%r617, %r596;
	mov.b32 	%r618, %r596;
	mov.b32 	%r619, %r596;
	mov.b32 	%r620, %r596;
	mov.b32 	%r621, %r596;
	mov.b32 	%r622, %r596;
	mov.b32 	%r623, %r596;
	mov.b32 	%r624, %r596;
	mov.b32 	%r625, %r596;
	mov.b32 	%r626, %r596;
	mov.b32 	%r627, %r596;
	mov.b32 	%r628, %r596;
	mov.b32 	%r629, %r596;
	mov.b32 	%r630, %r596;
	mov.b32 	%r631, %r596;
	mov.b32 	%r632, %r596;
	mov.b32 	%r633, %r596;
	mov.b32 	%r634, %r596;
	mov.b32 	%r635, %r596;
	mov.b32 	%r636, %r596;
	mov.b32 	%r637, %r596;
	mov.b32 	%r638, %r596;
	mov.b32 	%r639, %r596;
	mov.b32 	%r640, %r596;
	mov.b32 	%r641, %r596;
	mov.b32 	%r642, %r596;
	mov.b32 	%r643, %r596;
	mov.b32 	%r644, %r596;
	mov.b32 	%r645, %r596;
	mov.b32 	%r646, %r596;
	mov.b32 	%r647, %r596;
	mov.b32 	%r648, %r596;
	mov.b32 	%r649, %r596;
	mov.b32 	%r650, %r596;
	mov.b32 	%r651, %r596;
	mov.b32 	%r652, %r596;
	mov.b32 	%r653, %r596;
	mov.b32 	%r654, %r596;
	mov.b32 	%r655, %r596;
	mov.b32 	%r656, %r596;
	mov.b32 	%r657, %r596;
	mov.b32 	%r658, %r596;
	mov.b32 	%r659, %r596;
$L__BB0_2:                              // %.lr.ph
                                        // =>This Inner Loop Header: Depth=1
	.loc	1 32 20                         // matmul.py:32:20
	// begin inline asm
	mov.u32 %r338, 0x0;
	ld.global.b32 { %r338 }, [ %rd80 + 0 ];
	// end inline asm
	.loc	1 33 29                         // matmul.py:33:29
	mul.wide.s32 	%rd30, %r531, 4;
	add.s64 	%rd14, %rd1, %rd30;
	add.s64 	%rd15, %rd14, 16;
	add.s64 	%rd16, %rd14, 32;
	add.s64 	%rd17, %rd14, 48;
	add.s64 	%rd18, %rd14, 64;
	add.s64 	%rd19, %rd14, 80;
	add.s64 	%rd20, %rd14, 96;
	add.s64 	%rd21, %rd14, 112;
	add.s64 	%rd22, %rd14, 128;
	add.s64 	%rd23, %rd14, 144;
	add.s64 	%rd24, %rd14, 160;
	add.s64 	%rd25, %rd14, 176;
	add.s64 	%rd26, %rd14, 192;
	add.s64 	%rd27, %rd14, 208;
	add.s64 	%rd28, %rd14, 224;
	add.s64 	%rd29, %rd14, 240;
	.loc	1 33 20                         // matmul.py:33:20
	// begin inline asm
	mov.u32 %r339, 0x0;
	mov.u32 %r340, 0x0;
	mov.u32 %r341, 0x0;
	mov.u32 %r342, 0x0;
	ld.global.v4.b32 { %r339, %r340, %r341, %r342 }, [ %rd14 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r343, 0x0;
	mov.u32 %r344, 0x0;
	mov.u32 %r345, 0x0;
	mov.u32 %r346, 0x0;
	ld.global.v4.b32 { %r343, %r344, %r345, %r346 }, [ %rd15 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r347, 0x0;
	mov.u32 %r348, 0x0;
	mov.u32 %r349, 0x0;
	mov.u32 %r350, 0x0;
	ld.global.v4.b32 { %r347, %r348, %r349, %r350 }, [ %rd16 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r351, 0x0;
	mov.u32 %r352, 0x0;
	mov.u32 %r353, 0x0;
	mov.u32 %r354, 0x0;
	ld.global.v4.b32 { %r351, %r352, %r353, %r354 }, [ %rd17 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r355, 0x0;
	mov.u32 %r356, 0x0;
	mov.u32 %r357, 0x0;
	mov.u32 %r358, 0x0;
	ld.global.v4.b32 { %r355, %r356, %r357, %r358 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r359, 0x0;
	mov.u32 %r360, 0x0;
	mov.u32 %r361, 0x0;
	mov.u32 %r362, 0x0;
	ld.global.v4.b32 { %r359, %r360, %r361, %r362 }, [ %rd19 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r363, 0x0;
	mov.u32 %r364, 0x0;
	mov.u32 %r365, 0x0;
	mov.u32 %r366, 0x0;
	ld.global.v4.b32 { %r363, %r364, %r365, %r366 }, [ %rd20 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r367, 0x0;
	mov.u32 %r368, 0x0;
	mov.u32 %r369, 0x0;
	mov.u32 %r370, 0x0;
	ld.global.v4.b32 { %r367, %r368, %r369, %r370 }, [ %rd21 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r371, 0x0;
	mov.u32 %r372, 0x0;
	mov.u32 %r373, 0x0;
	mov.u32 %r374, 0x0;
	ld.global.v4.b32 { %r371, %r372, %r373, %r374 }, [ %rd22 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r375, 0x0;
	mov.u32 %r376, 0x0;
	mov.u32 %r377, 0x0;
	mov.u32 %r378, 0x0;
	ld.global.v4.b32 { %r375, %r376, %r377, %r378 }, [ %rd23 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r379, 0x0;
	mov.u32 %r380, 0x0;
	mov.u32 %r381, 0x0;
	mov.u32 %r382, 0x0;
	ld.global.v4.b32 { %r379, %r380, %r381, %r382 }, [ %rd24 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r383, 0x0;
	mov.u32 %r384, 0x0;
	mov.u32 %r385, 0x0;
	mov.u32 %r386, 0x0;
	ld.global.v4.b32 { %r383, %r384, %r385, %r386 }, [ %rd25 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r387, 0x0;
	mov.u32 %r388, 0x0;
	mov.u32 %r389, 0x0;
	mov.u32 %r390, 0x0;
	ld.global.v4.b32 { %r387, %r388, %r389, %r390 }, [ %rd26 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r391, 0x0;
	mov.u32 %r392, 0x0;
	mov.u32 %r393, 0x0;
	mov.u32 %r394, 0x0;
	ld.global.v4.b32 { %r391, %r392, %r393, %r394 }, [ %rd27 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r395, 0x0;
	mov.u32 %r396, 0x0;
	mov.u32 %r397, 0x0;
	mov.u32 %r398, 0x0;
	ld.global.v4.b32 { %r395, %r396, %r397, %r398 }, [ %rd28 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r399, 0x0;
	mov.u32 %r400, 0x0;
	mov.u32 %r401, 0x0;
	mov.u32 %r402, 0x0;
	ld.global.v4.b32 { %r399, %r400, %r401, %r402 }, [ %rd29 + 0 ];
	// end inline asm
	.loc	1 34 23                         // matmul.py:34:23
	fma.rn.f32 	%r599, %r338, %r342, %r599;
	fma.rn.f32 	%r598, %r338, %r341, %r598;
	fma.rn.f32 	%r597, %r338, %r340, %r597;
	fma.rn.f32 	%r596, %r338, %r339, %r596;
	fma.rn.f32 	%r604, %r338, %r343, %r604;
	fma.rn.f32 	%r605, %r338, %r344, %r605;
	fma.rn.f32 	%r606, %r338, %r345, %r606;
	fma.rn.f32 	%r607, %r338, %r346, %r607;
	fma.rn.f32 	%r603, %r338, %r374, %r603;
	fma.rn.f32 	%r602, %r338, %r373, %r602;
	fma.rn.f32 	%r601, %r338, %r372, %r601;
	fma.rn.f32 	%r600, %r338, %r371, %r600;
	fma.rn.f32 	%r608, %r338, %r375, %r608;
	fma.rn.f32 	%r609, %r338, %r376, %r609;
	fma.rn.f32 	%r610, %r338, %r377, %r610;
	fma.rn.f32 	%r611, %r338, %r378, %r611;
	fma.rn.f32 	%r612, %r338, %r347, %r612;
	fma.rn.f32 	%r613, %r338, %r348, %r613;
	fma.rn.f32 	%r614, %r338, %r349, %r614;
	fma.rn.f32 	%r615, %r338, %r350, %r615;
	fma.rn.f32 	%r616, %r338, %r379, %r616;
	fma.rn.f32 	%r617, %r338, %r380, %r617;
	fma.rn.f32 	%r618, %r338, %r381, %r618;
	fma.rn.f32 	%r619, %r338, %r382, %r619;
	fma.rn.f32 	%r620, %r338, %r351, %r620;
	fma.rn.f32 	%r621, %r338, %r352, %r621;
	fma.rn.f32 	%r622, %r338, %r353, %r622;
	fma.rn.f32 	%r623, %r338, %r354, %r623;
	fma.rn.f32 	%r624, %r338, %r383, %r624;
	fma.rn.f32 	%r625, %r338, %r384, %r625;
	fma.rn.f32 	%r626, %r338, %r385, %r626;
	fma.rn.f32 	%r627, %r338, %r386, %r627;
	fma.rn.f32 	%r628, %r338, %r355, %r628;
	fma.rn.f32 	%r629, %r338, %r356, %r629;
	fma.rn.f32 	%r630, %r338, %r357, %r630;
	fma.rn.f32 	%r631, %r338, %r358, %r631;
	fma.rn.f32 	%r632, %r338, %r387, %r632;
	fma.rn.f32 	%r633, %r338, %r388, %r633;
	fma.rn.f32 	%r634, %r338, %r389, %r634;
	fma.rn.f32 	%r635, %r338, %r390, %r635;
	fma.rn.f32 	%r636, %r338, %r359, %r636;
	fma.rn.f32 	%r637, %r338, %r360, %r637;
	fma.rn.f32 	%r638, %r338, %r361, %r638;
	fma.rn.f32 	%r639, %r338, %r362, %r639;
	fma.rn.f32 	%r640, %r338, %r391, %r640;
	fma.rn.f32 	%r641, %r338, %r392, %r641;
	fma.rn.f32 	%r642, %r338, %r393, %r642;
	fma.rn.f32 	%r643, %r338, %r394, %r643;
	fma.rn.f32 	%r644, %r338, %r363, %r644;
	fma.rn.f32 	%r645, %r338, %r364, %r645;
	fma.rn.f32 	%r646, %r338, %r365, %r646;
	fma.rn.f32 	%r647, %r338, %r366, %r647;
	fma.rn.f32 	%r648, %r338, %r395, %r648;
	fma.rn.f32 	%r649, %r338, %r396, %r649;
	fma.rn.f32 	%r650, %r338, %r397, %r650;
	fma.rn.f32 	%r651, %r338, %r398, %r651;
	fma.rn.f32 	%r652, %r338, %r367, %r652;
	fma.rn.f32 	%r653, %r338, %r368, %r653;
	fma.rn.f32 	%r654, %r338, %r369, %r654;
	fma.rn.f32 	%r655, %r338, %r370, %r655;
	fma.rn.f32 	%r656, %r338, %r399, %r656;
	fma.rn.f32 	%r657, %r338, %r400, %r657;
	fma.rn.f32 	%r658, %r338, %r401, %r658;
	fma.rn.f32 	%r659, %r338, %r402, %r659;
	.loc	1 30 19                         // matmul.py:30:19
	add.s64 	%rd81, %rd81, -1;
	add.s32 	%r531, %r531, %r203;
	add.s64 	%rd80, %rd80, 4;
	setp.ne.s64 	%p2, %rd81, 0;
	@%p2 bra 	$L__BB0_2;
$L__BB0_3:                              // %._crit_edge
	.loc	1 21 49                         // matmul.py:21:49
	shl.b32 	%r467, %r2, 2;
	and.b32 	%r468, %r467, 60;
	.loc	1 21 36                         // matmul.py:21:36
	or.b32 	%r469, %r468, %r4;
	.loc	1 20 49                         // matmul.py:20:49
	and.b32 	%r470, %r2, 112;
	bfe.u32 	%r471, %r2, 4, 3;
	.loc	1 20 36                         // matmul.py:20:36
	or.b32 	%r472, %r1, %r471;
	or.b32 	%r473, %r472, 120;
	or.b32 	%r474, %r472, 112;
	or.b32 	%r475, %r472, 104;
	or.b32 	%r476, %r472, 96;
	or.b32 	%r477, %r472, 88;
	or.b32 	%r478, %r472, 80;
	or.b32 	%r479, %r472, 72;
	or.b32 	%r480, %r472, 64;
	or.b32 	%r481, %r472, 56;
	or.b32 	%r482, %r472, 48;
	or.b32 	%r483, %r472, 40;
	or.b32 	%r484, %r472, 32;
	or.b32 	%r485, %r472, 24;
	or.b32 	%r486, %r472, 16;
	or.b32 	%r487, %r472, 8;
	.loc	1 37 39                         // matmul.py:37:39
	mul.lo.s32 	%r488, %r204, %r472;
	shl.b32 	%r489, %r204, 3;
	add.s32 	%r490, %r488, %r489;
	add.s32 	%r491, %r490, %r489;
	add.s32 	%r492, %r491, %r489;
	add.s32 	%r493, %r492, %r489;
	add.s32 	%r494, %r493, %r489;
	add.s32 	%r495, %r494, %r489;
	add.s32 	%r496, %r495, %r489;
	add.s32 	%r497, %r496, %r489;
	add.s32 	%r498, %r497, %r489;
	add.s32 	%r499, %r498, %r489;
	add.s32 	%r500, %r499, %r489;
	add.s32 	%r501, %r500, %r489;
	add.s32 	%r502, %r501, %r489;
	add.s32 	%r503, %r502, %r489;
	add.s32 	%r504, %r503, %r489;
	.loc	1 37 21                         // matmul.py:37:21
	mul.wide.s32 	%rd47, %r488, 4;
	add.s64 	%rd48, %rd9, %rd47;
	mul.wide.s32 	%rd49, %r490, 4;
	add.s64 	%rd50, %rd9, %rd49;
	mul.wide.s32 	%rd51, %r491, 4;
	add.s64 	%rd52, %rd9, %rd51;
	mul.wide.s32 	%rd53, %r492, 4;
	add.s64 	%rd54, %rd9, %rd53;
	mul.wide.s32 	%rd55, %r493, 4;
	add.s64 	%rd56, %rd9, %rd55;
	mul.wide.s32 	%rd57, %r494, 4;
	add.s64 	%rd58, %rd9, %rd57;
	mul.wide.s32 	%rd59, %r495, 4;
	add.s64 	%rd60, %rd9, %rd59;
	mul.wide.s32 	%rd61, %r496, 4;
	add.s64 	%rd62, %rd9, %rd61;
	mul.wide.s32 	%rd63, %r497, 4;
	add.s64 	%rd64, %rd9, %rd63;
	mul.wide.s32 	%rd65, %r498, 4;
	add.s64 	%rd66, %rd9, %rd65;
	mul.wide.s32 	%rd67, %r499, 4;
	add.s64 	%rd68, %rd9, %rd67;
	mul.wide.s32 	%rd69, %r500, 4;
	add.s64 	%rd70, %rd9, %rd69;
	mul.wide.s32 	%rd71, %r501, 4;
	add.s64 	%rd72, %rd9, %rd71;
	mul.wide.s32 	%rd73, %r502, 4;
	add.s64 	%rd74, %rd9, %rd73;
	mul.wide.s32 	%rd75, %r503, 4;
	add.s64 	%rd76, %rd9, %rd75;
	mul.wide.s32 	%rd77, %r504, 4;
	add.s64 	%rd78, %rd9, %rd77;
	.loc	1 37 51                         // matmul.py:37:51
	mul.wide.s32 	%rd79, %r469, 4;
	add.s64 	%rd31, %rd48, %rd79;
	add.s64 	%rd32, %rd50, %rd79;
	add.s64 	%rd33, %rd52, %rd79;
	add.s64 	%rd34, %rd54, %rd79;
	add.s64 	%rd35, %rd56, %rd79;
	add.s64 	%rd36, %rd58, %rd79;
	add.s64 	%rd37, %rd60, %rd79;
	add.s64 	%rd38, %rd62, %rd79;
	add.s64 	%rd39, %rd64, %rd79;
	add.s64 	%rd40, %rd66, %rd79;
	add.s64 	%rd41, %rd68, %rd79;
	add.s64 	%rd42, %rd70, %rd79;
	add.s64 	%rd43, %rd72, %rd79;
	add.s64 	%rd44, %rd74, %rd79;
	add.s64 	%rd45, %rd76, %rd79;
	add.s64 	%rd46, %rd78, %rd79;
	.loc	1 40 33                         // matmul.py:40:33
	setp.lt.s32 	%p19, %r472, %r199;
	setp.lt.s32 	%p20, %r487, %r199;
	setp.lt.s32 	%p21, %r486, %r199;
	setp.lt.s32 	%p22, %r485, %r199;
	setp.lt.s32 	%p23, %r484, %r199;
	setp.lt.s32 	%p24, %r483, %r199;
	setp.lt.s32 	%p25, %r482, %r199;
	setp.lt.s32 	%p26, %r481, %r199;
	setp.lt.s32 	%p27, %r480, %r199;
	setp.lt.s32 	%p28, %r479, %r199;
	setp.lt.s32 	%p29, %r478, %r199;
	setp.lt.s32 	%p30, %r477, %r199;
	setp.lt.s32 	%p31, %r476, %r199;
	setp.lt.s32 	%p32, %r475, %r199;
	setp.lt.s32 	%p33, %r474, %r199;
	setp.lt.s32 	%p34, %r473, %r199;
	.loc	1 40 58                         // matmul.py:40:58
	setp.lt.s32 	%p35, %r469, %r201;
	.loc	1 40 39                         // matmul.py:40:39
	and.pred 	%p3, %p19, %p35;
	and.pred 	%p4, %p20, %p35;
	and.pred 	%p5, %p21, %p35;
	and.pred 	%p6, %p22, %p35;
	and.pred 	%p7, %p23, %p35;
	and.pred 	%p8, %p24, %p35;
	and.pred 	%p9, %p25, %p35;
	and.pred 	%p10, %p26, %p35;
	and.pred 	%p11, %p27, %p35;
	and.pred 	%p12, %p28, %p35;
	and.pred 	%p13, %p29, %p35;
	and.pred 	%p14, %p30, %p35;
	and.pred 	%p15, %p31, %p35;
	and.pred 	%p16, %p32, %p35;
	and.pred 	%p17, %p33, %p35;
	and.pred 	%p18, %p34, %p35;
	.loc	1 41 21                         // matmul.py:41:21
	and.b32 	%r505, %r2, 7;
	shl.b32 	%r506, %r505, 12;
	shl.b32 	%r507, %r3, 4;
	or.b32 	%r508, %r506, %r507;
	mov.b32 	%r509, global_smem;
	add.s32 	%r510, %r509, %r508;
	st.shared.v4.b32 	[%r510], {%r596, %r597, %r598, %r599};
	st.shared.v4.b32 	[%r510+2048], {%r600, %r601, %r602, %r603};
	xor.b32 	%r511, %r508, 16;
	add.s32 	%r512, %r509, %r511;
	st.shared.v4.b32 	[%r512], {%r604, %r605, %r606, %r607};
	st.shared.v4.b32 	[%r512+2048], {%r608, %r609, %r610, %r611};
	xor.b32 	%r513, %r508, 32;
	add.s32 	%r514, %r509, %r513;
	st.shared.v4.b32 	[%r514], {%r612, %r613, %r614, %r615};
	st.shared.v4.b32 	[%r514+2048], {%r616, %r617, %r618, %r619};
	xor.b32 	%r515, %r508, 48;
	add.s32 	%r516, %r509, %r515;
	st.shared.v4.b32 	[%r516], {%r620, %r621, %r622, %r623};
	st.shared.v4.b32 	[%r516+2048], {%r624, %r625, %r626, %r627};
	xor.b32 	%r517, %r508, 64;
	add.s32 	%r518, %r509, %r517;
	st.shared.v4.b32 	[%r518], {%r628, %r629, %r630, %r631};
	st.shared.v4.b32 	[%r518+2048], {%r632, %r633, %r634, %r635};
	xor.b32 	%r519, %r508, 80;
	add.s32 	%r520, %r509, %r519;
	st.shared.v4.b32 	[%r520], {%r636, %r637, %r638, %r639};
	st.shared.v4.b32 	[%r520+2048], {%r640, %r641, %r642, %r643};
	xor.b32 	%r521, %r508, 96;
	add.s32 	%r522, %r509, %r521;
	st.shared.v4.b32 	[%r522], {%r644, %r645, %r646, %r647};
	st.shared.v4.b32 	[%r522+2048], {%r648, %r649, %r650, %r651};
	xor.b32 	%r523, %r508, 112;
	add.s32 	%r524, %r509, %r523;
	st.shared.v4.b32 	[%r524], {%r652, %r653, %r654, %r655};
	st.shared.v4.b32 	[%r524+2048], {%r656, %r657, %r658, %r659};
	bar.sync 	0;
	shl.b32 	%r525, %r2, 8;
	and.b32 	%r526, %r525, 30720;
	shl.b32 	%r527, %r505, 4;
	or.b32 	%r528, %r526, %r527;
	xor.b32 	%r529, %r528, %r470;
	add.s32 	%r530, %r509, %r529;
	ld.shared.v4.b32 	{%r403, %r404, %r405, %r406}, [%r530];
	ld.shared.v4.b32 	{%r407, %r408, %r409, %r410}, [%r530+128];
	ld.shared.v4.b32 	{%r411, %r412, %r413, %r414}, [%r530+256];
	ld.shared.v4.b32 	{%r415, %r416, %r417, %r418}, [%r530+384];
	ld.shared.v4.b32 	{%r419, %r420, %r421, %r422}, [%r530+512];
	ld.shared.v4.b32 	{%r423, %r424, %r425, %r426}, [%r530+640];
	ld.shared.v4.b32 	{%r427, %r428, %r429, %r430}, [%r530+768];
	ld.shared.v4.b32 	{%r431, %r432, %r433, %r434}, [%r530+896];
	ld.shared.v4.b32 	{%r435, %r436, %r437, %r438}, [%r530+1024];
	ld.shared.v4.b32 	{%r439, %r440, %r441, %r442}, [%r530+1152];
	ld.shared.v4.b32 	{%r443, %r444, %r445, %r446}, [%r530+1280];
	ld.shared.v4.b32 	{%r447, %r448, %r449, %r450}, [%r530+1408];
	ld.shared.v4.b32 	{%r451, %r452, %r453, %r454}, [%r530+1536];
	ld.shared.v4.b32 	{%r455, %r456, %r457, %r458}, [%r530+1664];
	ld.shared.v4.b32 	{%r459, %r460, %r461, %r462}, [%r530+1792];
	ld.shared.v4.b32 	{%r463, %r464, %r465, %r466}, [%r530+1920];
	// begin inline asm
	@%p3 st.global.v4.b32 [ %rd31 + 0 ], { %r403, %r404, %r405, %r406 };
	// end inline asm
	// begin inline asm
	@%p4 st.global.v4.b32 [ %rd32 + 0 ], { %r407, %r408, %r409, %r410 };
	// end inline asm
	// begin inline asm
	@%p5 st.global.v4.b32 [ %rd33 + 0 ], { %r411, %r412, %r413, %r414 };
	// end inline asm
	// begin inline asm
	@%p6 st.global.v4.b32 [ %rd34 + 0 ], { %r415, %r416, %r417, %r418 };
	// end inline asm
	// begin inline asm
	@%p7 st.global.v4.b32 [ %rd35 + 0 ], { %r419, %r420, %r421, %r422 };
	// end inline asm
	// begin inline asm
	@%p8 st.global.v4.b32 [ %rd36 + 0 ], { %r423, %r424, %r425, %r426 };
	// end inline asm
	// begin inline asm
	@%p9 st.global.v4.b32 [ %rd37 + 0 ], { %r427, %r428, %r429, %r430 };
	// end inline asm
	// begin inline asm
	@%p10 st.global.v4.b32 [ %rd38 + 0 ], { %r431, %r432, %r433, %r434 };
	// end inline asm
	// begin inline asm
	@%p11 st.global.v4.b32 [ %rd39 + 0 ], { %r435, %r436, %r437, %r438 };
	// end inline asm
	// begin inline asm
	@%p12 st.global.v4.b32 [ %rd40 + 0 ], { %r439, %r440, %r441, %r442 };
	// end inline asm
	// begin inline asm
	@%p13 st.global.v4.b32 [ %rd41 + 0 ], { %r443, %r444, %r445, %r446 };
	// end inline asm
	// begin inline asm
	@%p14 st.global.v4.b32 [ %rd42 + 0 ], { %r447, %r448, %r449, %r450 };
	// end inline asm
	// begin inline asm
	@%p15 st.global.v4.b32 [ %rd43 + 0 ], { %r451, %r452, %r453, %r454 };
	// end inline asm
	// begin inline asm
	@%p16 st.global.v4.b32 [ %rd44 + 0 ], { %r455, %r456, %r457, %r458 };
	// end inline asm
	// begin inline asm
	@%p17 st.global.v4.b32 [ %rd45 + 0 ], { %r459, %r460, %r461, %r462 };
	// end inline asm
	// begin inline asm
	@%p18 st.global.v4.b32 [ %rd46 + 0 ], { %r463, %r464, %r465, %r466 };
	// end inline asm
	.loc	1 41 4                          // matmul.py:41:4
	ret;
$L__tmp1:
$L__func_end0:
                                        // -- End function
}
	.file	1 "/home/ubuntu/triton/matmul.py"
	.section	.debug_abbrev
	{
.b8 1                                   // Abbreviation Code
.b8 17                                  // DW_TAG_compile_unit
.b8 0                                   // DW_CHILDREN_no
.b8 37                                  // DW_AT_producer
.b8 8                                   // DW_FORM_string
.b8 19                                  // DW_AT_language
.b8 5                                   // DW_FORM_data2
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 16                                  // DW_AT_stmt_list
.b8 6                                   // DW_FORM_data4
.b8 27                                  // DW_AT_comp_dir
.b8 8                                   // DW_FORM_string
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 0                                   // EOM(3)
	}
	.section	.debug_info
	{
.b32 51                                 // Length of Unit
.b8 2                                   // DWARF version number
.b8 0
.b32 .debug_abbrev                      // Offset Into Abbrev. Section
.b8 8                                   // Address Size (in bytes)
.b8 1                                   // Abbrev [1] 0xb:0x2c DW_TAG_compile_unit
.b8 116                                 // DW_AT_producer
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2                                   // DW_AT_language
.b8 0
.b8 109                                 // DW_AT_name
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line                        // DW_AT_stmt_list
.b8 47                                  // DW_AT_comp_dir
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 117
.b8 98
.b8 117
.b8 110
.b8 116
.b8 117
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
	}
	.section	.debug_macinfo	{	}
